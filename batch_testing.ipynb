{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e819a2",
   "metadata": {},
   "source": [
    "# Asynchronous batching\n",
    "\n",
    "This notebook shows how to use Asynchronous Batching to halve the cost of LM calls.\n",
    "\n",
    "Implementation relies on LiteLLM's own batch sdk ([link to docs](https://docs.litellm.ai/docs/batches))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eccb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b68a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "gpt_4_1 = dspy.LM(\"openai/gpt-4.1\", api_key=api_key)\n",
    "dspy.configure(lm=gpt_4_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d79c6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'question': '1+1?'}) (input_keys={'question'}),\n",
       " Example({'question': '2+2?'}) (input_keys={'question'}),\n",
       " Example({'question': '3+3?'}) (input_keys={'question'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = [\"1+1?\", \"2+2?\", \"3+3?\"]\n",
    "examples = [\n",
    "    dspy.Example(\n",
    "        question=q\n",
    "    ).with_inputs(\"question\")\n",
    "    for q in questions\n",
    "]\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7f6cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mathbot = dspy.Predict(\"question -> answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04e02c",
   "metadata": {},
   "source": [
    "## First method (fine-grained steps)\n",
    "\n",
    "The steps are:\n",
    "1. create a jsonl file (`module.create_batch_file()`) \n",
    "2. send it to the endpoint (`module.asubmit_batch_file()`) \n",
    "3. optionally listen for its completion (`module.aretrieve_batch()`) \n",
    "4. and finally retrieve the predictions (`mathbot.aretrieve_batch_predictions()`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89e61b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchRequestArtifacts(request_file=WindowsPath('batches/math_input.jsonl'), metadata_file=WindowsPath('batches/math_input.jsonl.metadata.json'), metadata={'generated_at': '2025-11-29T09:51:45.915000Z', 'request_file': 'batches\\\\math_input.jsonl', 'endpoint': '/v1/chat/completions', 'dependency_versions': {'python': '3.13', 'dspy': '3.1.0b1', 'cloudpickle': '3.1'}, 'module': {'class_path': 'dspy.predict.predict.Predict', 'repr': \"Predict(StringSignature(question -> answer\\n    instructions='Given the fields `question`, produce the fields `answer`.'\\n    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\\n    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\\n))\"}, 'lm': {'model': 'gpt-4.1', 'provider': 'openai', 'type': 'chat'}, 'examples': [{'custom_id': 'predict-0-a018f472', 'index': 0, 'inputs': {'question': '1+1?'}}, {'custom_id': 'predict-1-a863a8e4', 'index': 1, 'inputs': {'question': '2+2?'}}, {'custom_id': 'predict-2-8cb70f05', 'index': 2, 'inputs': {'question': '3+3?'}}], 'signature': {'instructions': 'Given the fields `question`, produce the fields `answer`.', 'fields': [{'prefix': 'Question:', 'description': '${question}'}, {'prefix': 'Answer:', 'description': '${answer}'}]}}, provider_name='openai', model_name='gpt-4.1', endpoint='/v1/chat/completions')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. create a jsonl file\n",
    "artifacts = mathbot.create_batch_file(\n",
    "    examples,\n",
    "    input_file_path=\"batches/math_input.jsonl\",\n",
    ")\n",
    "artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "846ecb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. send it to the endpoint\n",
    "batch_handle = await mathbot.asubmit_batch_file(\n",
    "    artifacts,\n",
    "    completion_window=\"24h\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18a0f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. optionally listen for its completion\n",
    "import asyncio\n",
    "\n",
    "while True:\n",
    "    info = await mathbot.aretrieve_batch(batch_handle.batch_id)\n",
    "    if getattr(info, \"status\", None) == \"completed\" and getattr(info, \"output_file_id\", None):\n",
    "        break\n",
    "    await asyncio.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c4510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. and finally retrieve the predictions\n",
    "predictions = await mathbot.aretrieve_batch_predictions(\n",
    "    batch_handle.batch_id,\n",
    "    artifacts,\n",
    "    download_output_path=\"batches/math_outputs.jsonl\",  # optional, defaults next to metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1a82edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(\n",
       "     answer='2'\n",
       " ),\n",
       " Prediction(\n",
       "     answer='2+2 equals 4.'\n",
       " ),\n",
       " Prediction(\n",
       "     answer='3+3 equals 6.'\n",
       " )]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5070b",
   "metadata": {},
   "source": [
    "## Second method (the lazy one-liner)\n",
    "\n",
    "`module.abatch()` is a wrapper around the previous steps. It doesn't create files on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db3d406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = await mathbot.abatch(\n",
    "    examples=examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5438d2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(\n",
       "     answer='1+1 = 2'\n",
       " ),\n",
       " Prediction(\n",
       "     answer='2+2 equals 4.'\n",
       " ),\n",
       " Prediction(\n",
       "     answer='3+3=6'\n",
       " )]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
