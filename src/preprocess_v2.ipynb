{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ef50a5",
   "metadata": {},
   "source": [
    "# Fuel-Segment Preprocessing\n",
    "\n",
    "This notebook keeps only the fields DSPy needs for each fuel segment:\n",
    "`idx`, `flight_id`, `fuel_kg`, `flight_date`, `aircraft_type`, `origin_name`,\n",
    "`destination_name`, plus a compact `track_summary`, the readable\n",
    "`track_points_compact` string, and the `vertical_rate_balance` structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d6796e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\rayte\\Work\\prc2025dspy\n",
      "Fuel file: fuel_final_submission.parquet\n",
      "Flight list file: flightlist_final.parquet\n",
      "Flight tracks folder: c:\\Users\\rayte\\Work\\prc2025dspy\\data\\flights_final\n",
      "Output parquet: c:\\Users\\rayte\\Work\\prc2025dspy\\data\\final\\llm_segments.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Literal, Optional\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pl.Config.set_tbl_rows(20)\n",
    "\n",
    "\n",
    "def _locate_data_root(start: Path) -> Path:\n",
    "    for candidate in (start, *start.parents):\n",
    "        data_dir = candidate / \"data\"\n",
    "        if data_dir.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Unable to locate the `data/` directory relative to this notebook.\")\n",
    "\n",
    "\n",
    "PROJECT_ROOT = _locate_data_root(Path.cwd())\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "DATA_TYPE: Literal[\"train\", \"rank\", \"final\"] = \"final\"\n",
    "\n",
    "fuel_filename = \"fuel_train.parquet\" if DATA_TYPE == \"train\" else f\"fuel_{DATA_TYPE}_submission.parquet\"\n",
    "FUEL_FILE = DATA_DIR / fuel_filename\n",
    "FLIGHTLIST_FILE = DATA_DIR / f\"flightlist_{DATA_TYPE}.parquet\"\n",
    "FLIGHTS_DIR = DATA_DIR / f\"flights_{DATA_TYPE}\"\n",
    "OUTPUT_FILE = DATA_DIR / DATA_TYPE / f\"llm_segments.parquet\"\n",
    "\n",
    "for required in (FUEL_FILE, FLIGHTLIST_FILE):\n",
    "    if not required.exists():\n",
    "        raise FileNotFoundError(required)\n",
    "FLIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Fuel file: {FUEL_FILE.name}\")\n",
    "print(f\"Flight list file: {FLIGHTLIST_FILE.name}\")\n",
    "print(f\"Flight tracks folder: {FLIGHTS_DIR}\")\n",
    "print(f\"Output parquet: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e5ca3",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Everything required to summarise track points lives in this notebook so the\n",
    "preprocessing does not depend on `prc_challenge` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67e1ea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_NUMERIC_TRACK_COLUMNS = (\"altitude\", \"groundspeed\", \"vertical_rate\", \"mach\", \"TAS\", \"CAS\")\n",
    "_PHASE_TO_CODE = {\"climb\": 1, \"descent\": -1, \"cruise\": 0, \"level\": 0, \"mixed\": 2, \"unknown\": 99}\n",
    "TRACK_POINT_COLUMNS = [\n",
    "    \"timestamp\",\n",
    "    \"source\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"altitude\",\n",
    "    \"groundspeed\",\n",
    "    \"vertical_rate\",\n",
    "    \"mach\",\n",
    "    \"TAS\",\n",
    "    \"CAS\",\n",
    "]\n",
    "\n",
    "\n",
    "def _format_number(value: Any) -> str:\n",
    "    if value is None:\n",
    "        return \"NA\"\n",
    "    try:\n",
    "        numeric = float(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return str(value)\n",
    "    if math.isnan(numeric):\n",
    "        return \"NA\"\n",
    "    return format(numeric, \".4g\")\n",
    "\n",
    "\n",
    "def _parse_timestamp(value: Any) -> Optional[datetime]:\n",
    "    if isinstance(value, datetime):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return datetime.fromisoformat(value)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _sample_indices(length: int, sample_size: int) -> List[int]:\n",
    "    if length <= 0:\n",
    "        return []\n",
    "    if sample_size <= 1 or length == 1:\n",
    "        return [0]\n",
    "    step = (length - 1) / max(sample_size - 1, 1)\n",
    "    indices: List[int] = []\n",
    "    for i in range(sample_size):\n",
    "        idx = int(round(i * step))\n",
    "        if idx >= length:\n",
    "            idx = length - 1\n",
    "        if not indices or idx != indices[-1]:\n",
    "            indices.append(idx)\n",
    "    if indices[-1] != length - 1:\n",
    "        indices.append(length - 1)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def _clean_numeric_series(track_points: List[Dict[str, Any]], key: str) -> List[float]:\n",
    "    values: List[float] = []\n",
    "    for point in track_points:\n",
    "        if not isinstance(point, dict):\n",
    "            continue\n",
    "        value = point.get(key)\n",
    "        if value is None:\n",
    "            continue\n",
    "        try:\n",
    "            numeric = float(value)\n",
    "        except (TypeError, ValueError):\n",
    "            continue\n",
    "        if math.isnan(numeric):\n",
    "            continue\n",
    "        values.append(numeric)\n",
    "    return values\n",
    "\n",
    "\n",
    "def _safe_mean(values: List[float]) -> Optional[float]:\n",
    "    if not values:\n",
    "        return None\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "\n",
    "def _safe_std(values: List[float], mean: Optional[float]) -> Optional[float]:\n",
    "    if mean is None or len(values) < 2:\n",
    "        return None\n",
    "    variance = sum((value - mean) ** 2 for value in values) / (len(values) - 1)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "\n",
    "def _summarise_track_points(track_points: List[Dict[str, Any]], sample_count: int = 4) -> Dict[str, Any]:\n",
    "    summary: Dict[str, Any] = {\n",
    "        \"num_points\": len(track_points),\n",
    "        \"time_window\": None,\n",
    "        \"source_counts\": {},\n",
    "        \"numeric_profiles\": {},\n",
    "        \"path_profile\": None,\n",
    "        \"aggregate_features\": {},\n",
    "        \"vertical_rate_balance\": None,\n",
    "        \"phase_hint\": \"unknown\",\n",
    "    }\n",
    "    if not track_points:\n",
    "        return summary\n",
    "\n",
    "    start_ts = track_points[0].get(\"timestamp\")\n",
    "    end_ts = track_points[-1].get(\"timestamp\")\n",
    "    start_dt = _parse_timestamp(start_ts)\n",
    "    end_dt = _parse_timestamp(end_ts)\n",
    "    time_window: Dict[str, Any] = {\"start\": start_ts, \"end\": end_ts, \"minutes\": None}\n",
    "    if start_dt and end_dt:\n",
    "        duration_minutes = (end_dt - start_dt).total_seconds() / 60.0\n",
    "        time_window[\"minutes\"] = round(duration_minutes, 3)\n",
    "    summary[\"time_window\"] = time_window\n",
    "\n",
    "    source_counts = Counter((point.get(\"source\") or \"unknown\") for point in track_points)\n",
    "    summary[\"source_counts\"] = dict(source_counts)\n",
    "\n",
    "    numeric_profiles: Dict[str, Dict[str, Any]] = {}\n",
    "    aggregates: Dict[str, float] = {}\n",
    "    vertical_balance: Optional[Dict[str, float]] = None\n",
    "    for column in _NUMERIC_TRACK_COLUMNS:\n",
    "        series = _clean_numeric_series(track_points, column)\n",
    "        if not series:\n",
    "            continue\n",
    "        indices = _sample_indices(len(series), sample_count)\n",
    "        samples = [round(series[i], 4) for i in indices]\n",
    "        delta = series[-1] - series[0]\n",
    "        col_min = min(series)\n",
    "        col_max = max(series)\n",
    "        mean_val = _safe_mean(series)\n",
    "        std_val = _safe_std(series, mean_val)\n",
    "        profile: Dict[str, Any] = {\n",
    "            \"samples\": samples,\n",
    "            \"delta\": round(delta, 4),\n",
    "            \"range\": round(col_max - col_min, 4),\n",
    "            \"min\": round(col_min, 4),\n",
    "            \"max\": round(col_max, 4),\n",
    "        }\n",
    "        if mean_val is not None:\n",
    "            profile[\"mean\"] = round(mean_val, 4)\n",
    "        if std_val is not None:\n",
    "            profile[\"std\"] = round(std_val, 4)\n",
    "        numeric_profiles[column] = profile\n",
    "        aggregates[f\"{column}_delta\"] = round(delta, 4)\n",
    "        aggregates[f\"{column}_range\"] = round(col_max - col_min, 4)\n",
    "        if mean_val is not None:\n",
    "            aggregates[f\"{column}_mean\"] = round(mean_val, 4)\n",
    "        if std_val is not None:\n",
    "            aggregates[f\"{column}_std\"] = round(std_val, 4)\n",
    "        if column == \"vertical_rate\":\n",
    "            threshold = 64.0\n",
    "            positives = sum(1 for value in series if value > threshold)\n",
    "            negatives = sum(1 for value in series if value < -threshold)\n",
    "            total = len(series)\n",
    "            zeros = total - positives - negatives\n",
    "            vertical_balance = {\n",
    "                \"positive_frac\": round(positives / total, 4),\n",
    "                \"negative_frac\": round(negatives / total, 4),\n",
    "                \"near_zero_frac\": round(max(zeros, 0) / total, 4),\n",
    "            }\n",
    "            aggregates[\"vertical_rate_positive_frac\"] = vertical_balance[\"positive_frac\"]\n",
    "            aggregates[\"vertical_rate_negative_frac\"] = vertical_balance[\"negative_frac\"]\n",
    "            aggregates[\"vertical_rate_near_zero_frac\"] = vertical_balance[\"near_zero_frac\"]\n",
    "    summary[\"numeric_profiles\"] = numeric_profiles\n",
    "    summary[\"aggregate_features\"] = aggregates\n",
    "    summary[\"vertical_rate_balance\"] = vertical_balance\n",
    "    summary[\"phase_hint\"] = _infer_phase(summary)\n",
    "    aggregates[\"phase_hint_code\"] = _PHASE_TO_CODE.get(summary[\"phase_hint\"], 99)\n",
    "\n",
    "    lat_series = _clean_numeric_series(track_points, \"latitude\")\n",
    "    lon_series = _clean_numeric_series(track_points, \"longitude\")\n",
    "    if lat_series and lon_series:\n",
    "        idxs = _sample_indices(len(lat_series), sample_count)\n",
    "        path_profile = {\n",
    "            \"lat_samples\": [round(lat_series[i], 4) for i in idxs],\n",
    "            \"lon_samples\": [round(lon_series[i], 4) for i in idxs],\n",
    "            \"delta_lat\": round(lat_series[-1] - lat_series[0], 4),\n",
    "            \"delta_lon\": round(lon_series[-1] - lon_series[0], 4),\n",
    "        }\n",
    "        summary[\"path_profile\"] = path_profile\n",
    "        aggregates[\"delta_lat\"] = path_profile[\"delta_lat\"]\n",
    "        aggregates[\"delta_lon\"] = path_profile[\"delta_lon\"]\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def _infer_phase(summary: Dict[str, Any]) -> str:\n",
    "    aggregates = summary.get(\"aggregate_features\") or {}\n",
    "    numeric = summary.get(\"numeric_profiles\") or {}\n",
    "    altitude_profile = numeric.get(\"altitude\") or {}\n",
    "    vertical_profile = numeric.get(\"vertical_rate\") or {}\n",
    "    delta_alt = altitude_profile.get(\"delta\")\n",
    "    if delta_alt is None:\n",
    "        delta_alt = aggregates.get(\"altitude_delta\")\n",
    "    mean_vr = vertical_profile.get(\"mean\")\n",
    "    vrange = vertical_profile.get(\"range\")\n",
    "    balance = summary.get(\"vertical_rate_balance\") or {}\n",
    "    pos_frac = balance.get(\"positive_frac\") if balance else None\n",
    "    neg_frac = balance.get(\"negative_frac\") if balance else None\n",
    "    mean_vr = mean_vr if mean_vr is not None else aggregates.get(\"vertical_rate_mean\")\n",
    "    delta_alt = delta_alt if delta_alt is not None else 0.0\n",
    "    mean_vr = mean_vr if mean_vr is not None else 0.0\n",
    "    if delta_alt > 800 or mean_vr > 150:\n",
    "        return \"climb\"\n",
    "    if delta_alt < -800 or mean_vr < -150:\n",
    "        return \"descent\"\n",
    "    if vrange is not None and vrange < 200 and abs(mean_vr) < 80:\n",
    "        return \"cruise\"\n",
    "    if pos_frac is not None and neg_frac is not None and pos_frac > 0.2 and neg_frac > 0.2:\n",
    "        return \"mixed\"\n",
    "    return \"level\"\n",
    "\n",
    "\n",
    "def _format_track_summary(summary: Dict[str, Any]) -> str:\n",
    "    if not summary.get(\"num_points\"):\n",
    "        return \"no track points\"\n",
    "    parts: List[str] = []\n",
    "    time_window = summary.get(\"time_window\") or {}\n",
    "    start = time_window.get(\"start\")\n",
    "    end = time_window.get(\"end\")\n",
    "    minutes = time_window.get(\"minutes\")\n",
    "    if start and end:\n",
    "        segment = f\"time {start}->{end}\"\n",
    "        if minutes is not None:\n",
    "            segment += f\" ({_format_number(minutes)} min)\"\n",
    "        parts.append(segment)\n",
    "    sources = summary.get(\"source_counts\") or {}\n",
    "    if sources:\n",
    "        source_text = \", \".join(f\"{src}:{count}\" for src, count in sorted(sources.items()))\n",
    "        parts.append(f\"sources {source_text}\")\n",
    "    numeric_profiles = summary.get(\"numeric_profiles\") or {}\n",
    "    for column in (\"altitude\", \"groundspeed\", \"vertical_rate\", \"mach\"):\n",
    "        stats = numeric_profiles.get(column)\n",
    "        if not stats:\n",
    "            continue\n",
    "        samples = stats.get(\"samples\") or []\n",
    "        sample_text = \" -> \".join(_format_number(val) for val in samples) if samples else \"n/a\"\n",
    "        extras = []\n",
    "        delta = stats.get(\"delta\")\n",
    "        if delta is not None:\n",
    "            extras.append(f\"delta {_format_number(delta)}\")\n",
    "        value_range = stats.get(\"range\")\n",
    "        if value_range is not None:\n",
    "            extras.append(f\"range {_format_number(value_range)}\")\n",
    "        mean_val = stats.get(\"mean\")\n",
    "        if mean_val is not None:\n",
    "            extras.append(f\"mean {_format_number(mean_val)}\")\n",
    "        if extras:\n",
    "            parts.append(f\"{column} {sample_text} ({', '.join(extras)})\")\n",
    "        else:\n",
    "            parts.append(f\"{column} {sample_text}\")\n",
    "    path_profile = summary.get(\"path_profile\") or {}\n",
    "    lat_samples = path_profile.get(\"lat_samples\")\n",
    "    lon_samples = path_profile.get(\"lon_samples\")\n",
    "    if lat_samples and lon_samples:\n",
    "        pairs = \" -> \".join(\n",
    "            f\"{_format_number(lat)}/{_format_number(lon)}\"\n",
    "            for lat, lon in zip(lat_samples, lon_samples)\n",
    "        )\n",
    "        parts.append(f\"path {pairs}\")\n",
    "    delta_lat = path_profile.get(\"delta_lat\")\n",
    "    delta_lon = path_profile.get(\"delta_lon\")\n",
    "    if delta_lat is not None or delta_lon is not None:\n",
    "        parts.append(f\"delta_lat {_format_number(delta_lat)} delta_lon {_format_number(delta_lon)}\")\n",
    "    phase = summary.get(\"phase_hint\")\n",
    "    if phase:\n",
    "        parts.append(f\"phase {phase}\")\n",
    "    balance = summary.get(\"vertical_rate_balance\") or {}\n",
    "    if balance:\n",
    "        parts.append(\n",
    "            \"vr balance +{pos:.2f} / -{neg:.2f} / ~0 {zero:.2f}\".format(\n",
    "                pos=balance.get(\"positive_frac\", 0.0),\n",
    "                neg=balance.get(\"negative_frac\", 0.0),\n",
    "                zero=balance.get(\"near_zero_frac\", 0.0),\n",
    "            )\n",
    "        )\n",
    "    compact_text = \" | \".join(parts) if parts else \"track summary unavailable\"\n",
    "    return compact_text[:600] + (\"...\" if len(compact_text) > 600 else \"\")\n",
    "\n",
    "\n",
    "def _slice_track_window(track_df: Optional[pl.DataFrame], start: datetime, end: datetime) -> pl.DataFrame:\n",
    "    if track_df is None or track_df.is_empty():\n",
    "        return pl.DataFrame()\n",
    "    available_cols = [col for col in TRACK_POINT_COLUMNS if col in track_df.columns]\n",
    "    if not available_cols:\n",
    "        return pl.DataFrame()\n",
    "    return (\n",
    "        track_df\n",
    "        .filter((pl.col(\"timestamp\") >= start) & (pl.col(\"timestamp\") <= end))\n",
    "        .select(available_cols)\n",
    "        .sort(\"timestamp\")\n",
    "    )\n",
    "\n",
    "\n",
    "def _prepare_track_points(df: pl.DataFrame) -> List[Dict[str, Any]]:\n",
    "    if df.is_empty():\n",
    "        return []\n",
    "    records = df.to_dicts()\n",
    "    for record in records:\n",
    "        ts = record.get(\"timestamp\")\n",
    "        if hasattr(ts, \"isoformat\"):\n",
    "            record[\"timestamp\"] = ts.isoformat()\n",
    "        record.setdefault(\"source\", \"unknown\")\n",
    "    return records\n",
    "\n",
    "\n",
    "def summarise_segment_track(track_df: Optional[pl.DataFrame], start: datetime, end: datetime) -> Dict[str, Any]:\n",
    "    window = _slice_track_window(track_df, start, end)\n",
    "    track_points = _prepare_track_points(window)\n",
    "    return _summarise_track_points(track_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6481ee",
   "metadata": {},
   "source": [
    "## Load fuel segments and flight metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c27fbe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segments: 61,745 across 4,724 flights\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>idx</th><th>flight_id</th><th>fuel_kg</th><th>flight_date</th><th>aircraft_type</th><th>origin_name</th><th>destination_name</th><th>start</th><th>end</th></tr><tr><td>i64</td><td>str</td><td>null</td><td>date</td><td>str</td><td>str</td><td>str</td><td>datetime[ns]</td><td>datetime[ns]</td></tr></thead><tbody><tr><td>0</td><td>&quot;prc806615763&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-09-01 03:03:10.925</td><td>2025-09-01 03:07:51.584</td></tr><tr><td>1</td><td>&quot;prc806615763&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-09-01 03:07:51.584</td><td>2025-09-01 03:12:50.921</td></tr><tr><td>2</td><td>&quot;prc806615763&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-09-01 03:12:50.921</td><td>2025-09-01 03:17:51.404</td></tr><tr><td>3</td><td>&quot;prc806615763&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-09-01 03:17:51.404</td><td>2025-09-01 03:22:50.539</td></tr><tr><td>4</td><td>&quot;prc806615763&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-09-01 03:22:50.539</td><td>2025-09-01 03:27:50.727</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌─────┬─────────────┬─────────┬────────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ idx ┆ flight_id   ┆ fuel_kg ┆ flight_dat ┆ … ┆ origin_nam ┆ destinatio ┆ start      ┆ end        │\n",
       "│ --- ┆ ---         ┆ ---     ┆ e          ┆   ┆ e          ┆ n_name     ┆ ---        ┆ ---        │\n",
       "│ i64 ┆ str         ┆ null    ┆ ---        ┆   ┆ ---        ┆ ---        ┆ datetime[n ┆ datetime[n │\n",
       "│     ┆             ┆         ┆ date       ┆   ┆ str        ┆ str        ┆ s]         ┆ s]         │\n",
       "╞═════╪═════════════╪═════════╪════════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 0   ┆ prc80661576 ┆ null    ┆ null       ┆ … ┆ null       ┆ null       ┆ 2025-09-01 ┆ 2025-09-01 │\n",
       "│     ┆ 3           ┆         ┆            ┆   ┆            ┆            ┆ 03:03:10.9 ┆ 03:07:51.5 │\n",
       "│     ┆             ┆         ┆            ┆   ┆            ┆            ┆ 25         ┆ 84         │\n",
       "│ 1   ┆ prc80661576 ┆ null    ┆ null       ┆ … ┆ null       ┆ null       ┆ 2025-09-01 ┆ 2025-09-01 │\n",
       "│     ┆ 3           ┆         ┆            ┆   ┆            ┆            ┆ 03:07:51.5 ┆ 03:12:50.9 │\n",
       "│     ┆             ┆         ┆            ┆   ┆            ┆            ┆ 84         ┆ 21         │\n",
       "│ 2   ┆ prc80661576 ┆ null    ┆ null       ┆ … ┆ null       ┆ null       ┆ 2025-09-01 ┆ 2025-09-01 │\n",
       "│     ┆ 3           ┆         ┆            ┆   ┆            ┆            ┆ 03:12:50.9 ┆ 03:17:51.4 │\n",
       "│     ┆             ┆         ┆            ┆   ┆            ┆            ┆ 21         ┆ 04         │\n",
       "│ 3   ┆ prc80661576 ┆ null    ┆ null       ┆ … ┆ null       ┆ null       ┆ 2025-09-01 ┆ 2025-09-01 │\n",
       "│     ┆ 3           ┆         ┆            ┆   ┆            ┆            ┆ 03:17:51.4 ┆ 03:22:50.5 │\n",
       "│     ┆             ┆         ┆            ┆   ┆            ┆            ┆ 04         ┆ 39         │\n",
       "│ 4   ┆ prc80661576 ┆ null    ┆ null       ┆ … ┆ null       ┆ null       ┆ 2025-09-01 ┆ 2025-09-01 │\n",
       "│     ┆ 3           ┆         ┆            ┆   ┆            ┆            ┆ 03:22:50.5 ┆ 03:27:50.7 │\n",
       "│     ┆             ┆         ┆            ┆   ┆            ┆            ┆ 39         ┆ 27         │\n",
       "└─────┴─────────────┴─────────┴────────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fuel_df = pl.read_parquet(FUEL_FILE).select([\"idx\", \"flight_id\", \"start\", \"end\", \"fuel_kg\"])\n",
    "flightlist_df = pl.read_parquet(FLIGHTLIST_FILE).select([\n",
    "    \"flight_id\",\n",
    "    \"flight_date\",\n",
    "    \"aircraft_type\",\n",
    "    \"origin_name\",\n",
    "    \"destination_name\",\n",
    "])\n",
    "\n",
    "segments_df = (\n",
    "    fuel_df\n",
    "    .join(flightlist_df, on=\"flight_id\", how=\"left\")\n",
    "    .select([\n",
    "        \"idx\",\n",
    "        \"flight_id\",\n",
    "        \"fuel_kg\",\n",
    "        \"flight_date\",\n",
    "        \"aircraft_type\",\n",
    "        \"origin_name\",\n",
    "        \"destination_name\",\n",
    "        \"start\",\n",
    "        \"end\",\n",
    "    ])\n",
    "    .sort([\"flight_id\", \"start\"])\n",
    ")\n",
    "\n",
    "print(f\"Segments: {segments_df.height:,} across {segments_df['flight_id'].n_unique():,} flights\")\n",
    "segments_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb16ef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (61_745, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>idx</th><th>flight_id</th><th>start</th><th>end</th><th>fuel_kg</th></tr><tr><td>i64</td><td>str</td><td>datetime[ns]</td><td>datetime[ns]</td><td>null</td></tr></thead><tbody><tr><td>0</td><td>&quot;prc806615763&quot;</td><td>2025-09-01 03:03:10.925</td><td>2025-09-01 03:07:51.584</td><td>null</td></tr><tr><td>1</td><td>&quot;prc806615763&quot;</td><td>2025-09-01 03:07:51.584</td><td>2025-09-01 03:12:50.921</td><td>null</td></tr><tr><td>2</td><td>&quot;prc806615763&quot;</td><td>2025-09-01 03:12:50.921</td><td>2025-09-01 03:17:51.404</td><td>null</td></tr><tr><td>3</td><td>&quot;prc806615763&quot;</td><td>2025-09-01 03:17:51.404</td><td>2025-09-01 03:22:50.539</td><td>null</td></tr><tr><td>4</td><td>&quot;prc806615763&quot;</td><td>2025-09-01 03:22:50.539</td><td>2025-09-01 03:27:50.727</td><td>null</td></tr><tr><td>5</td><td>&quot;prc806615763&quot;</td><td>2025-09-01 03:27:50.727</td><td>2025-09-01 03:32:50.802</td><td>null</td></tr><tr><td>6</td><td>&quot;prc806615763&quot;</td><td>2025-09-01 03:32:50.802</td><td>2025-09-01 03:37:51.778</td><td>null</td></tr><tr><td>7</td><td>&quot;prc806615763&quot;</td><td>2025-09-01 03:37:51.778</td><td>2025-09-01 03:42:50.706</td><td>null</td></tr><tr><td>8</td><td>&quot;prc806615763&quot;</td><td>2025-09-01 03:42:50.706</td><td>2025-09-01 03:47:50.452</td><td>null</td></tr><tr><td>9</td><td>&quot;prc806615763&quot;</td><td>2025-09-01 03:47:50.452</td><td>2025-09-01 03:52:51.183</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>37446</td><td>&quot;prc821916593&quot;</td><td>2025-10-31 22:41:33.494</td><td>2025-10-31 22:46:40.096</td><td>null</td></tr><tr><td>37447</td><td>&quot;prc821916593&quot;</td><td>2025-10-31 22:46:40.096</td><td>2025-10-31 22:51:43.503</td><td>null</td></tr><tr><td>37448</td><td>&quot;prc821916593&quot;</td><td>2025-10-31 22:51:43.503</td><td>2025-10-31 22:56:48.484</td><td>null</td></tr><tr><td>37449</td><td>&quot;prc821916593&quot;</td><td>2025-10-31 22:56:48.484</td><td>2025-10-31 23:01:53.466</td><td>null</td></tr><tr><td>37450</td><td>&quot;prc821916593&quot;</td><td>2025-10-31 23:01:53.466</td><td>2025-10-31 23:06:58.703</td><td>null</td></tr><tr><td>37451</td><td>&quot;prc821916593&quot;</td><td>2025-10-31 23:06:58.703</td><td>2025-10-31 23:17:08.526</td><td>null</td></tr><tr><td>37452</td><td>&quot;prc821916593&quot;</td><td>2025-10-31 23:17:08.526</td><td>2025-10-31 23:22:13.468</td><td>null</td></tr><tr><td>37453</td><td>&quot;prc821916593&quot;</td><td>2025-10-31 23:22:13.468</td><td>2025-10-31 23:27:18.458</td><td>null</td></tr><tr><td>37454</td><td>&quot;prc821916593&quot;</td><td>2025-10-31 23:27:18.458</td><td>2025-10-31 23:52:43.483</td><td>null</td></tr><tr><td>37455</td><td>&quot;prc821916593&quot;</td><td>2025-10-31 23:52:43.483</td><td>2025-10-31 23:57:49.145</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (61_745, 5)\n",
       "┌───────┬──────────────┬─────────────────────────┬─────────────────────────┬─────────┐\n",
       "│ idx   ┆ flight_id    ┆ start                   ┆ end                     ┆ fuel_kg │\n",
       "│ ---   ┆ ---          ┆ ---                     ┆ ---                     ┆ ---     │\n",
       "│ i64   ┆ str          ┆ datetime[ns]            ┆ datetime[ns]            ┆ null    │\n",
       "╞═══════╪══════════════╪═════════════════════════╪═════════════════════════╪═════════╡\n",
       "│ 0     ┆ prc806615763 ┆ 2025-09-01 03:03:10.925 ┆ 2025-09-01 03:07:51.584 ┆ null    │\n",
       "│ 1     ┆ prc806615763 ┆ 2025-09-01 03:07:51.584 ┆ 2025-09-01 03:12:50.921 ┆ null    │\n",
       "│ 2     ┆ prc806615763 ┆ 2025-09-01 03:12:50.921 ┆ 2025-09-01 03:17:51.404 ┆ null    │\n",
       "│ 3     ┆ prc806615763 ┆ 2025-09-01 03:17:51.404 ┆ 2025-09-01 03:22:50.539 ┆ null    │\n",
       "│ 4     ┆ prc806615763 ┆ 2025-09-01 03:22:50.539 ┆ 2025-09-01 03:27:50.727 ┆ null    │\n",
       "│ 5     ┆ prc806615763 ┆ 2025-09-01 03:27:50.727 ┆ 2025-09-01 03:32:50.802 ┆ null    │\n",
       "│ 6     ┆ prc806615763 ┆ 2025-09-01 03:32:50.802 ┆ 2025-09-01 03:37:51.778 ┆ null    │\n",
       "│ 7     ┆ prc806615763 ┆ 2025-09-01 03:37:51.778 ┆ 2025-09-01 03:42:50.706 ┆ null    │\n",
       "│ 8     ┆ prc806615763 ┆ 2025-09-01 03:42:50.706 ┆ 2025-09-01 03:47:50.452 ┆ null    │\n",
       "│ 9     ┆ prc806615763 ┆ 2025-09-01 03:47:50.452 ┆ 2025-09-01 03:52:51.183 ┆ null    │\n",
       "│ …     ┆ …            ┆ …                       ┆ …                       ┆ …       │\n",
       "│ 37446 ┆ prc821916593 ┆ 2025-10-31 22:41:33.494 ┆ 2025-10-31 22:46:40.096 ┆ null    │\n",
       "│ 37447 ┆ prc821916593 ┆ 2025-10-31 22:46:40.096 ┆ 2025-10-31 22:51:43.503 ┆ null    │\n",
       "│ 37448 ┆ prc821916593 ┆ 2025-10-31 22:51:43.503 ┆ 2025-10-31 22:56:48.484 ┆ null    │\n",
       "│ 37449 ┆ prc821916593 ┆ 2025-10-31 22:56:48.484 ┆ 2025-10-31 23:01:53.466 ┆ null    │\n",
       "│ 37450 ┆ prc821916593 ┆ 2025-10-31 23:01:53.466 ┆ 2025-10-31 23:06:58.703 ┆ null    │\n",
       "│ 37451 ┆ prc821916593 ┆ 2025-10-31 23:06:58.703 ┆ 2025-10-31 23:17:08.526 ┆ null    │\n",
       "│ 37452 ┆ prc821916593 ┆ 2025-10-31 23:17:08.526 ┆ 2025-10-31 23:22:13.468 ┆ null    │\n",
       "│ 37453 ┆ prc821916593 ┆ 2025-10-31 23:22:13.468 ┆ 2025-10-31 23:27:18.458 ┆ null    │\n",
       "│ 37454 ┆ prc821916593 ┆ 2025-10-31 23:27:18.458 ┆ 2025-10-31 23:52:43.483 ┆ null    │\n",
       "│ 37455 ┆ prc821916593 ┆ 2025-10-31 23:52:43.483 ┆ 2025-10-31 23:57:49.145 ┆ null    │\n",
       "└───────┴──────────────┴─────────────────────────┴─────────────────────────┴─────────┘"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6775e",
   "metadata": {},
   "source": [
    "## Build the compact dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88861cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee51ecac0424ecf91cb1dadc6ef780c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarising segments:   0%|          | 0/4724 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 61,745 rows to c:\\Users\\rayte\\Work\\prc2025dspy\\data\\final\\llm_segments.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>idx</th><th>flight_id</th><th>fuel_kg</th><th>flight_date</th><th>aircraft_type</th><th>origin_name</th><th>destination_name</th><th>start</th><th>end</th><th>track_points_compact</th><th>vertical_rate_balance</th></tr><tr><td>i64</td><td>str</td><td>f64</td><td>date</td><td>str</td><td>str</td><td>str</td><td>datetime[ms]</td><td>datetime[ms]</td><td>str</td><td>struct[3]</td></tr></thead><tbody><tr><td>0</td><td>&quot;prc806615763&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;no track points&quot;</td><td>null</td></tr><tr><td>1</td><td>&quot;prc806615763&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;no track points&quot;</td><td>null</td></tr><tr><td>2</td><td>&quot;prc806615763&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;no track points&quot;</td><td>null</td></tr><tr><td>3</td><td>&quot;prc806615763&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;no track points&quot;</td><td>null</td></tr><tr><td>4</td><td>&quot;prc806615763&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;no track points&quot;</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 11)\n",
       "┌─────┬─────────────┬─────────┬────────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ idx ┆ flight_id   ┆ fuel_kg ┆ flight_dat ┆ … ┆ start      ┆ end        ┆ track_poin ┆ vertical_r │\n",
       "│ --- ┆ ---         ┆ ---     ┆ e          ┆   ┆ ---        ┆ ---        ┆ ts_compact ┆ ate_balanc │\n",
       "│ i64 ┆ str         ┆ f64     ┆ ---        ┆   ┆ datetime[m ┆ datetime[m ┆ ---        ┆ e          │\n",
       "│     ┆             ┆         ┆ date       ┆   ┆ s]         ┆ s]         ┆ str        ┆ ---        │\n",
       "│     ┆             ┆         ┆            ┆   ┆            ┆            ┆            ┆ struct[3]  │\n",
       "╞═════╪═════════════╪═════════╪════════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 0   ┆ prc80661576 ┆ null    ┆ null       ┆ … ┆ null       ┆ null       ┆ no track   ┆ null       │\n",
       "│     ┆ 3           ┆         ┆            ┆   ┆            ┆            ┆ points     ┆            │\n",
       "│ 1   ┆ prc80661576 ┆ null    ┆ null       ┆ … ┆ null       ┆ null       ┆ no track   ┆ null       │\n",
       "│     ┆ 3           ┆         ┆            ┆   ┆            ┆            ┆ points     ┆            │\n",
       "│ 2   ┆ prc80661576 ┆ null    ┆ null       ┆ … ┆ null       ┆ null       ┆ no track   ┆ null       │\n",
       "│     ┆ 3           ┆         ┆            ┆   ┆            ┆            ┆ points     ┆            │\n",
       "│ 3   ┆ prc80661576 ┆ null    ┆ null       ┆ … ┆ null       ┆ null       ┆ no track   ┆ null       │\n",
       "│     ┆ 3           ┆         ┆            ┆   ┆            ┆            ┆ points     ┆            │\n",
       "│ 4   ┆ prc80661576 ┆ null    ┆ null       ┆ … ┆ null       ┆ null       ┆ no track   ┆ null       │\n",
       "│     ┆ 3           ┆         ┆            ┆   ┆            ┆            ┆ points     ┆            │\n",
       "└─────┴─────────────┴─────────┴────────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCHEMA = {\n",
    "    \"idx\": pl.Int64,\n",
    "    \"flight_id\": pl.Utf8,\n",
    "    \"fuel_kg\": pl.Float64,\n",
    "    \"flight_date\": pl.Date,\n",
    "    \"aircraft_type\": pl.Utf8,\n",
    "    \"origin_name\": pl.Utf8,\n",
    "    \"destination_name\": pl.Utf8,\n",
    "    \"start\": pl.Datetime(\"ms\"),\n",
    "    \"end\": pl.Datetime(\"ms\"),\n",
    "    \"track_points_compact\": pl.Utf8,\n",
    "    \"vertical_rate_balance\": pl.Struct([\n",
    "        pl.Field(\"positive_frac\", pl.Float64),\n",
    "        pl.Field(\"negative_frac\", pl.Float64),\n",
    "        pl.Field(\"near_zero_frac\", pl.Float64),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "def build_segment_dataset(segments: pl.DataFrame) -> pl.DataFrame:\n",
    "    grouped = defaultdict(list)\n",
    "    for row in segments.iter_rows(named=True):\n",
    "        grouped[row[\"flight_id\"]].append(row)\n",
    "\n",
    "    processed: List[Dict[str, Any]] = []\n",
    "    for flight_id, rows in tqdm(grouped.items(), desc=\"Summarising segments\", total=len(grouped)):\n",
    "        flight_path = FLIGHTS_DIR / f\"{flight_id}.parquet\"\n",
    "        track_df = None\n",
    "        if flight_path.exists():\n",
    "            track_df = pl.read_parquet(flight_path).sort(\"timestamp\")\n",
    "        for row in rows:\n",
    "            summary = summarise_segment_track(track_df, row[\"start\"], row[\"end\"])\n",
    "            processed.append(\n",
    "                {\n",
    "                    \"idx\": row[\"idx\"],\n",
    "                    \"flight_id\": row[\"flight_id\"],\n",
    "                    \"fuel_kg\": row[\"fuel_kg\"],\n",
    "                    \"flight_date\": row[\"flight_date\"],\n",
    "                    \"aircraft_type\": row[\"aircraft_type\"],\n",
    "                    \"origin_name\": row[\"origin_name\"],\n",
    "                    \"destination_name\": row[\"destination_name\"],\n",
    "                    # \"track_summary\": summary,\n",
    "                    \"track_points_compact\": _format_track_summary(summary),\n",
    "                    \"vertical_rate_balance\": summary.get(\"vertical_rate_balance\"),\n",
    "                }\n",
    "            )\n",
    "        del track_df\n",
    "    return pl.from_dicts(processed, schema=SCHEMA)\n",
    "\n",
    "segment_records = build_segment_dataset(segments_df)\n",
    "segment_records.write_parquet(OUTPUT_FILE)\n",
    "print(f\"Wrote {segment_records.height:,} rows to {OUTPUT_FILE}\")\n",
    "segment_records.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67430d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>track_points_compact</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;no track points&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1,)\n",
       "Series: 'track_points_compact' [str]\n",
       "[\n",
       "\t\"no track points\"\n",
       "]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_records[0][\"track_points_compact\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prc2025dspy (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
