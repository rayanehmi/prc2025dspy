{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ef50a5",
   "metadata": {},
   "source": [
    "# Fuel-Segment Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6796e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Literal, Optional, Union\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pl.Config.set_tbl_rows(20)\n",
    "\n",
    "\n",
    "def _locate_data_root(start: Path) -> Path:\n",
    "    for candidate in (start, *start.parents):\n",
    "        data_dir = candidate / \"data\"\n",
    "        if data_dir.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Unable to locate the `data/` directory relative to this notebook.\")\n",
    "\n",
    "\n",
    "PROJECT_ROOT = _locate_data_root(Path.cwd())\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "DATA_TYPE: Literal[\"train\", \"rank\", \"final\"] = \"train\"\n",
    "\n",
    "fuel_filename = \"fuel_train.parquet\" if DATA_TYPE == \"train\" else f\"fuel_{DATA_TYPE}_submission.parquet\"\n",
    "FUEL_FILE = DATA_DIR / fuel_filename\n",
    "FLIGHTLIST_FILE = DATA_DIR / f\"flightlist_{DATA_TYPE}.parquet\"\n",
    "FLIGHTS_DIR = DATA_DIR / f\"flights_{DATA_TYPE}\"\n",
    "OUTPUT_FILE = DATA_DIR / f\"llm_segments_{DATA_TYPE}.parquet\"\n",
    "\n",
    "for required in (FUEL_FILE, FLIGHTLIST_FILE):\n",
    "    if not required.exists():\n",
    "        raise FileNotFoundError(required)\n",
    "FLIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Fuel file: {FUEL_FILE.name}\")\n",
    "print(f\"Flight list file: {FLIGHTLIST_FILE.name}\")\n",
    "print(f\"Flight tracks folder: {FLIGHTS_DIR}\")\n",
    "print(f\"Output parquet: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e5ca3",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Everything required to summarise track points lives in this notebook so the\n",
    "preprocessing does not depend on `prc_challenge` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e1ea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_NUMERIC_TRACK_COLUMNS = (\"altitude\", \"groundspeed\", \"vertical_rate\", \"mach\", \"TAS\", \"CAS\")\n",
    "_PHASE_TO_CODE = {\"climb\": 1, \"descent\": -1, \"cruise\": 0, \"level\": 0, \"mixed\": 2, \"unknown\": 99}\n",
    "TRACK_POINT_COLUMNS = [\n",
    "    \"timestamp\",\n",
    "    \"source\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"altitude\",\n",
    "    \"groundspeed\",\n",
    "    \"vertical_rate\",\n",
    "    \"mach\",\n",
    "    \"TAS\",\n",
    "    \"CAS\",\n",
    "]\n",
    "\n",
    "TrackFrame = Union[pl.DataFrame, pl.LazyFrame]\n",
    "\n",
    "def _resolve_track_columns(track: TrackFrame) -> List[str]:\n",
    "    if isinstance(track, pl.LazyFrame):\n",
    "        column_names = track.collect_schema().names()\n",
    "    else:\n",
    "        column_names = track.columns\n",
    "    return [col for col in TRACK_POINT_COLUMNS if col in column_names]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _format_number(value: Any) -> str:\n",
    "    if value is None:\n",
    "        return \"NA\"\n",
    "    try:\n",
    "        numeric = float(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return str(value)\n",
    "    if math.isnan(numeric):\n",
    "        return \"NA\"\n",
    "    return format(numeric, \".4g\")\n",
    "\n",
    "\n",
    "def _parse_timestamp(value: Any) -> Optional[datetime]:\n",
    "    if isinstance(value, datetime):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return datetime.fromisoformat(value)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _sample_indices(length: int, sample_size: int) -> List[int]:\n",
    "    if length <= 0:\n",
    "        return []\n",
    "    if sample_size <= 1 or length == 1:\n",
    "        return [0]\n",
    "    step = (length - 1) / max(sample_size - 1, 1)\n",
    "    indices: List[int] = []\n",
    "    for i in range(sample_size):\n",
    "        idx = int(round(i * step))\n",
    "        if idx >= length:\n",
    "            idx = length - 1\n",
    "        if not indices or idx != indices[-1]:\n",
    "            indices.append(idx)\n",
    "    if indices[-1] != length - 1:\n",
    "        indices.append(length - 1)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def _clean_numeric_series(track_points: List[Dict[str, Any]], key: str) -> List[float]:\n",
    "    values: List[float] = []\n",
    "    for point in track_points:\n",
    "        if not isinstance(point, dict):\n",
    "            continue\n",
    "        value = point.get(key)\n",
    "        if value is None:\n",
    "            continue\n",
    "        try:\n",
    "            numeric = float(value)\n",
    "        except (TypeError, ValueError):\n",
    "            continue\n",
    "        if math.isnan(numeric):\n",
    "            continue\n",
    "        values.append(numeric)\n",
    "    return values\n",
    "\n",
    "\n",
    "def _safe_mean(values: List[float]) -> Optional[float]:\n",
    "    if not values:\n",
    "        return None\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "\n",
    "def _safe_std(values: List[float], mean: Optional[float]) -> Optional[float]:\n",
    "    if mean is None or len(values) < 2:\n",
    "        return None\n",
    "    variance = sum((value - mean) ** 2 for value in values) / (len(values) - 1)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "\n",
    "def _summarise_track_points(track_points: List[Dict[str, Any]], sample_count: int = 4) -> Dict[str, Any]:\n",
    "    summary: Dict[str, Any] = {\n",
    "        \"num_points\": len(track_points),\n",
    "        \"time_window\": None,\n",
    "        \"source_counts\": {},\n",
    "        \"numeric_profiles\": {},\n",
    "        \"path_profile\": None,\n",
    "        \"aggregate_features\": {},\n",
    "        \"vertical_rate_balance\": None,\n",
    "        \"phase_hint\": \"unknown\",\n",
    "    }\n",
    "    if not track_points:\n",
    "        return summary\n",
    "\n",
    "    start_ts = track_points[0].get(\"timestamp\")\n",
    "    end_ts = track_points[-1].get(\"timestamp\")\n",
    "    start_dt = _parse_timestamp(start_ts)\n",
    "    end_dt = _parse_timestamp(end_ts)\n",
    "    time_window: Dict[str, Any] = {\"start\": start_ts, \"end\": end_ts, \"minutes\": None}\n",
    "    if start_dt and end_dt:\n",
    "        duration_minutes = (end_dt - start_dt).total_seconds() / 60.0\n",
    "        time_window[\"minutes\"] = round(duration_minutes, 3)\n",
    "    summary[\"time_window\"] = time_window\n",
    "\n",
    "    source_counts = Counter((point.get(\"source\") or \"unknown\") for point in track_points)\n",
    "    summary[\"source_counts\"] = dict(source_counts)\n",
    "\n",
    "    numeric_profiles: Dict[str, Dict[str, Any]] = {}\n",
    "    aggregates: Dict[str, float] = {}\n",
    "    vertical_balance: Optional[Dict[str, float]] = None\n",
    "    for column in _NUMERIC_TRACK_COLUMNS:\n",
    "        series = _clean_numeric_series(track_points, column)\n",
    "        if not series:\n",
    "            continue\n",
    "        indices = _sample_indices(len(series), sample_count)\n",
    "        samples = [round(series[i], 4) for i in indices]\n",
    "        delta = series[-1] - series[0]\n",
    "        col_min = min(series)\n",
    "        col_max = max(series)\n",
    "        mean_val = _safe_mean(series)\n",
    "        std_val = _safe_std(series, mean_val)\n",
    "        profile: Dict[str, Any] = {\n",
    "            \"samples\": samples,\n",
    "            \"delta\": round(delta, 4),\n",
    "            \"range\": round(col_max - col_min, 4),\n",
    "            \"min\": round(col_min, 4),\n",
    "            \"max\": round(col_max, 4),\n",
    "        }\n",
    "        if mean_val is not None:\n",
    "            profile[\"mean\"] = round(mean_val, 4)\n",
    "        if std_val is not None:\n",
    "            profile[\"std\"] = round(std_val, 4)\n",
    "        numeric_profiles[column] = profile\n",
    "        aggregates[f\"{column}_delta\"] = round(delta, 4)\n",
    "        aggregates[f\"{column}_range\"] = round(col_max - col_min, 4)\n",
    "        if mean_val is not None:\n",
    "            aggregates[f\"{column}_mean\"] = round(mean_val, 4)\n",
    "        if std_val is not None:\n",
    "            aggregates[f\"{column}_std\"] = round(std_val, 4)\n",
    "        if column == \"vertical_rate\":\n",
    "            threshold = 64.0\n",
    "            positives = sum(1 for value in series if value > threshold)\n",
    "            negatives = sum(1 for value in series if value < -threshold)\n",
    "            total = len(series)\n",
    "            zeros = total - positives - negatives\n",
    "            vertical_balance = {\n",
    "                \"positive_frac\": round(positives / total, 4),\n",
    "                \"negative_frac\": round(negatives / total, 4),\n",
    "                \"near_zero_frac\": round(max(zeros, 0) / total, 4),\n",
    "            }\n",
    "            aggregates[\"vertical_rate_positive_frac\"] = vertical_balance[\"positive_frac\"]\n",
    "            aggregates[\"vertical_rate_negative_frac\"] = vertical_balance[\"negative_frac\"]\n",
    "            aggregates[\"vertical_rate_near_zero_frac\"] = vertical_balance[\"near_zero_frac\"]\n",
    "    summary[\"numeric_profiles\"] = numeric_profiles\n",
    "    summary[\"aggregate_features\"] = aggregates\n",
    "    summary[\"vertical_rate_balance\"] = vertical_balance\n",
    "    summary[\"phase_hint\"] = _infer_phase(summary)\n",
    "    aggregates[\"phase_hint_code\"] = _PHASE_TO_CODE.get(summary[\"phase_hint\"], 99)\n",
    "\n",
    "    lat_series = _clean_numeric_series(track_points, \"latitude\")\n",
    "    lon_series = _clean_numeric_series(track_points, \"longitude\")\n",
    "    if lat_series and lon_series:\n",
    "        idxs = _sample_indices(len(lat_series), sample_count)\n",
    "        path_profile = {\n",
    "            \"lat_samples\": [round(lat_series[i], 4) for i in idxs],\n",
    "            \"lon_samples\": [round(lon_series[i], 4) for i in idxs],\n",
    "            \"delta_lat\": round(lat_series[-1] - lat_series[0], 4),\n",
    "            \"delta_lon\": round(lon_series[-1] - lon_series[0], 4),\n",
    "        }\n",
    "        summary[\"path_profile\"] = path_profile\n",
    "        aggregates[\"delta_lat\"] = path_profile[\"delta_lat\"]\n",
    "        aggregates[\"delta_lon\"] = path_profile[\"delta_lon\"]\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def _infer_phase(summary: Dict[str, Any]) -> str:\n",
    "    aggregates = summary.get(\"aggregate_features\") or {}\n",
    "    numeric = summary.get(\"numeric_profiles\") or {}\n",
    "    altitude_profile = numeric.get(\"altitude\") or {}\n",
    "    vertical_profile = numeric.get(\"vertical_rate\") or {}\n",
    "    delta_alt = altitude_profile.get(\"delta\")\n",
    "    if delta_alt is None:\n",
    "        delta_alt = aggregates.get(\"altitude_delta\")\n",
    "    mean_vr = vertical_profile.get(\"mean\")\n",
    "    vrange = vertical_profile.get(\"range\")\n",
    "    balance = summary.get(\"vertical_rate_balance\") or {}\n",
    "    pos_frac = balance.get(\"positive_frac\") if balance else None\n",
    "    neg_frac = balance.get(\"negative_frac\") if balance else None\n",
    "    mean_vr = mean_vr if mean_vr is not None else aggregates.get(\"vertical_rate_mean\")\n",
    "    delta_alt = delta_alt if delta_alt is not None else 0.0\n",
    "    mean_vr = mean_vr if mean_vr is not None else 0.0\n",
    "    if delta_alt > 800 or mean_vr > 150:\n",
    "        return \"climb\"\n",
    "    if delta_alt < -800 or mean_vr < -150:\n",
    "        return \"descent\"\n",
    "    if vrange is not None and vrange < 200 and abs(mean_vr) < 80:\n",
    "        return \"cruise\"\n",
    "    if pos_frac is not None and neg_frac is not None and pos_frac > 0.2 and neg_frac > 0.2:\n",
    "        return \"mixed\"\n",
    "    return \"level\"\n",
    "\n",
    "\n",
    "def _format_track_summary(summary: Dict[str, Any]) -> str:\n",
    "    if not summary.get(\"num_points\"):\n",
    "        return \"no track points\"\n",
    "    parts: List[str] = []\n",
    "    time_window = summary.get(\"time_window\") or {}\n",
    "    start = time_window.get(\"start\")\n",
    "    end = time_window.get(\"end\")\n",
    "    minutes = time_window.get(\"minutes\")\n",
    "    if start and end:\n",
    "        segment = f\"time {start}->{end}\"\n",
    "        if minutes is not None:\n",
    "            segment += f\" ({_format_number(minutes)} min)\"\n",
    "        parts.append(segment)\n",
    "    sources = summary.get(\"source_counts\") or {}\n",
    "    if sources:\n",
    "        source_text = \", \".join(f\"{src}:{count}\" for src, count in sorted(sources.items()))\n",
    "        parts.append(f\"sources {source_text}\")\n",
    "    numeric_profiles = summary.get(\"numeric_profiles\") or {}\n",
    "    for column in (\"altitude\", \"groundspeed\", \"vertical_rate\", \"mach\"):\n",
    "        stats = numeric_profiles.get(column)\n",
    "        if not stats:\n",
    "            continue\n",
    "        samples = stats.get(\"samples\") or []\n",
    "        sample_text = \" -> \".join(_format_number(val) for val in samples) if samples else \"n/a\"\n",
    "        extras = []\n",
    "        delta = stats.get(\"delta\")\n",
    "        if delta is not None:\n",
    "            extras.append(f\"delta {_format_number(delta)}\")\n",
    "        value_range = stats.get(\"range\")\n",
    "        if value_range is not None:\n",
    "            extras.append(f\"range {_format_number(value_range)}\")\n",
    "        mean_val = stats.get(\"mean\")\n",
    "        if mean_val is not None:\n",
    "            extras.append(f\"mean {_format_number(mean_val)}\")\n",
    "        if extras:\n",
    "            parts.append(f\"{column} {sample_text} ({', '.join(extras)})\")\n",
    "        else:\n",
    "            parts.append(f\"{column} {sample_text}\")\n",
    "    path_profile = summary.get(\"path_profile\") or {}\n",
    "    lat_samples = path_profile.get(\"lat_samples\")\n",
    "    lon_samples = path_profile.get(\"lon_samples\")\n",
    "    if lat_samples and lon_samples:\n",
    "        pairs = \" -> \".join(\n",
    "            f\"{_format_number(lat)}/{_format_number(lon)}\"\n",
    "            for lat, lon in zip(lat_samples, lon_samples)\n",
    "        )\n",
    "        parts.append(f\"path {pairs}\")\n",
    "    delta_lat = path_profile.get(\"delta_lat\")\n",
    "    delta_lon = path_profile.get(\"delta_lon\")\n",
    "    if delta_lat is not None or delta_lon is not None:\n",
    "        parts.append(f\"delta_lat {_format_number(delta_lat)} delta_lon {_format_number(delta_lon)}\")\n",
    "    phase = summary.get(\"phase_hint\")\n",
    "    if phase:\n",
    "        parts.append(f\"phase {phase}\")\n",
    "    balance = summary.get(\"vertical_rate_balance\") or {}\n",
    "    if balance:\n",
    "        parts.append(\n",
    "            \"vr balance +{pos:.2f} / -{neg:.2f} / ~0 {zero:.2f}\".format(\n",
    "                pos=balance.get(\"positive_frac\", 0.0),\n",
    "                neg=balance.get(\"negative_frac\", 0.0),\n",
    "                zero=balance.get(\"near_zero_frac\", 0.0),\n",
    "            )\n",
    "        )\n",
    "    compact_text = \" | \".join(parts) if parts else \"track summary unavailable\"\n",
    "    return compact_text[:600] + (\"...\" if len(compact_text) > 600 else \"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _scan_track_lazy(path: Path) -> Optional[pl.LazyFrame]:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    scan = pl.scan_parquet(path)\n",
    "    schema_names = scan.collect_schema().names()\n",
    "    needed_cols = [col for col in TRACK_POINT_COLUMNS if col in schema_names]\n",
    "    if not needed_cols:\n",
    "        return None\n",
    "    return scan.select(needed_cols)\n",
    "\n",
    "\n",
    "def _slice_track_window(track: Optional[TrackFrame], start: datetime, end: datetime) -> pl.DataFrame:\n",
    "    if track is None:\n",
    "        return pl.DataFrame()\n",
    "    available_cols = _resolve_track_columns(track)\n",
    "    if not available_cols:\n",
    "        return pl.DataFrame()\n",
    "    time_filter = (pl.col(\"timestamp\") >= start) & (pl.col(\"timestamp\") <= end)\n",
    "    if isinstance(track, pl.LazyFrame):\n",
    "        return (\n",
    "            track\n",
    "            .filter(time_filter)\n",
    "            .select(available_cols)\n",
    "            .sort(\"timestamp\")\n",
    "            .collect(engine=\"streaming\")\n",
    "        )\n",
    "    if track.is_empty():\n",
    "        return pl.DataFrame()\n",
    "    return (\n",
    "        track\n",
    "        .filter(time_filter)\n",
    "        .select(available_cols)\n",
    "        .sort(\"timestamp\")\n",
    "    )\n",
    "\n",
    "\n",
    "def _prepare_track_points(df: pl.DataFrame) -> List[Dict[str, Any]]:\n",
    "    if df.is_empty():\n",
    "        return []\n",
    "    records = df.to_dicts()\n",
    "    for record in records:\n",
    "        ts = record.get(\"timestamp\")\n",
    "        if hasattr(ts, \"isoformat\"):\n",
    "            record[\"timestamp\"] = ts.isoformat()\n",
    "        record.setdefault(\"source\", \"unknown\")\n",
    "    return records\n",
    "\n",
    "\n",
    "def summarise_segment_track(track: Optional[TrackFrame], start: datetime, end: datetime) -> Dict[str, Any]:\n",
    "    window = _slice_track_window(track, start, end)\n",
    "    track_points = _prepare_track_points(window)\n",
    "    return _summarise_track_points(track_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6481ee",
   "metadata": {},
   "source": [
    "## Load fuel segments and flight metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27fbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fuel_df = pl.read_parquet(FUEL_FILE).select([\"idx\", \"flight_id\", \"start\", \"end\", \"fuel_kg\"])\n",
    "flightlist_df = pl.read_parquet(FLIGHTLIST_FILE).select([\n",
    "    \"flight_id\",\n",
    "    \"flight_date\",\n",
    "    \"aircraft_type\",\n",
    "    \"origin_name\",\n",
    "    \"destination_name\",\n",
    "])\n",
    "\n",
    "segments_df = (\n",
    "    fuel_df\n",
    "    .join(flightlist_df, on=\"flight_id\", how=\"left\")\n",
    "    .select([\n",
    "        \"idx\",\n",
    "        \"flight_id\",\n",
    "        \"fuel_kg\",\n",
    "        \"flight_date\",\n",
    "        \"aircraft_type\",\n",
    "        \"origin_name\",\n",
    "        \"destination_name\",\n",
    "        \"start\",\n",
    "        \"end\",\n",
    "    ])\n",
    "    .sort([\"flight_id\", \"start\"])\n",
    ")\n",
    "\n",
    "print(f\"Segments: {segments_df.height:,} across {segments_df['flight_id'].n_unique():,} flights\")\n",
    "segments_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6775e",
   "metadata": {},
   "source": [
    "## Build the compact dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88861cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_segment_dataset(segments: pl.DataFrame) -> pl.DataFrame:\n",
    "    grouped = defaultdict(list)\n",
    "    for row in segments.iter_rows(named=True):\n",
    "        grouped[row[\"flight_id\"]].append(row)\n",
    "\n",
    "    processed: List[Dict[str, Any]] = []\n",
    "    for flight_id, rows in tqdm(grouped.items(), desc=\"Summarising segments\", total=len(grouped)):\n",
    "        flight_path = FLIGHTS_DIR / f\"{flight_id}.parquet\"\n",
    "        track_lazy = _scan_track_lazy(flight_path)\n",
    "        for row in rows:\n",
    "            summary = summarise_segment_track(track_lazy, row[\"start\"], row[\"end\"])\n",
    "            processed.append(\n",
    "                {\n",
    "                    \"idx\": row[\"idx\"],\n",
    "                    \"flight_id\": row[\"flight_id\"],\n",
    "                    \"fuel_kg\": row[\"fuel_kg\"],\n",
    "                    \"flight_date\": row[\"flight_date\"],\n",
    "                    \"aircraft_type\": row[\"aircraft_type\"],\n",
    "                    \"origin_name\": row[\"origin_name\"],\n",
    "                    \"destination_name\": row[\"destination_name\"],\n",
    "                    # \"track_summary\": summary,\n",
    "                    \"track_points_compact\": _format_track_summary(summary),\n",
    "                    \"vertical_rate_balance\": summary.get(\"vertical_rate_balance\"),\n",
    "                }\n",
    "            )\n",
    "    return pl.from_dicts(processed)\n",
    "\n",
    "segment_records = build_segment_dataset(segments_df)\n",
    "segment_records.write_parquet(OUTPUT_FILE)\n",
    "print(f\"Wrote {segment_records.height:,} rows to {OUTPUT_FILE}\")\n",
    "segment_records.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67430d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time 2025-04-13T02:31:04.447000->2025-04-13T03:01:04.487000 (30 min) | sources acars:2, adsb:3076 | altitude 3.597e+04 -> 3.597e+04 -> 3.6e+04 -> 3.6e+04 (delta 24.99, range 24.99, mean 3.599e+04) | groundspeed 467 -> 474 -> 471 -> 476 (delta 9, range 15, mean 472) | vertical_rate 0 -> 0 -> 0 -> -64 (delta -64, range 128, mean -3.36) | mach 0.86 -> 0.86 (delta 0, range 0, mean 0.86) | path 45.18/24.35 -> 45.9/22.72 -> 46.62/20.97 -> 47.22/19.52 | delta_lat 2.033 delta_lon -4.833 | phase cruise | vr balance +0.00 / -0.00 / ~0 1.00'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_records[0][\"track_points_compact\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prc2025dspy (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
