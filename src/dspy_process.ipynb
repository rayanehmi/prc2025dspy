{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff52ef8",
   "metadata": {},
   "source": [
    "## DSPy process\n",
    "\n",
    "Note: I use my own fork of DSPy because I had to implement asynchronous batching: https://github.com/rayanehmi/dspy/tree/feat/async_batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18693820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "import os \n",
    "\n",
    "DATA_PATH = Path.cwd().parent / \"data\"\n",
    "DATA_TYPE : Literal[\"train\", \"rank\", \"final\"] = \"final\"\n",
    "OUTPUT_DIR = os.path.join(DATA_PATH, DATA_TYPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aef113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete data\n",
    "import polars as pl\n",
    "\n",
    "SEGMENTS_PATH = os.path.join(OUTPUT_DIR, \"llm_segments.parquet\")\n",
    "print(SEGMENTS_PATH)\n",
    "initial_df = pl.read_parquet(SEGMENTS_PATH)\n",
    "initial_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996abaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "gpt_4_1 = dspy.LM(\"openai/gpt-4.1\", api_key=api_key)\n",
    "gpt_4_1_nano = dspy.LM(\"openai/gpt-4.1-nano\", api_key=api_key)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "gpt_oss_120b = dspy.LM(\"groq/openai/gpt-oss-120b\", api_key=groq_api_key, cache=False)\n",
    "gpt_5_1_instant = dspy.LM(\"openai/gpt-5.1\", api_key=api_key, temperature=1.0, max_tokens=32000, reasoning_effort=\"none\", cache=False)\n",
    "\n",
    "# json_adapter = dspy.JSONAdapter()  # made the first batch fail?\n",
    "\n",
    "dspy.configure(lm=gpt_oss_120b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class BurntFuelPrediction(dspy.Signature):\n",
    "    \"\"\"Predict the amount of fuel burnt in kgs by the plane over the given segment of flight.\n",
    "    Segment data is constructed from noisy telemetry: use your common sense if values seem wrong.\n",
    "    hint: vertical_rate_balance contains positive_frac, negative_frac and near_zero_frac, each corresponding \n",
    "    to the share of samples where vertical rate is respectively more than, less than or around 64 ft/min.\n",
    "    hint 2: estimate the fuel weight penalty (heavy in the beginning, lighter in the end).\n",
    "    \"\"\"\n",
    "    features : dict[str, Any] = dspy.InputField()\n",
    "    fuel_kg : float = dspy.OutputField() \n",
    "\n",
    "# Zero-shot chain of thought\n",
    "fuel_cot = dspy.ChainOfThought(BurntFuelPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c651b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dspy_examples(row, with_fuel: bool = True):\n",
    "    \"\"\"Converts a row to a dspy.Example.\"\"\"\n",
    "    row_data = row.to_dicts()[0] if hasattr(row, \"to_dicts\") else row\n",
    "\n",
    "    def clean(value):\n",
    "        return \"\" if value is None else value\n",
    "\n",
    "    inputs = [\n",
    "        \"idx\",\n",
    "        \"aircraft_type\",\n",
    "        \"origin_name\",\n",
    "        \"origin_destination\",\n",
    "        \"track_points_compact\",\n",
    "        \"track_points_compact\",\n",
    "        \"vertical_rate_balance\"\n",
    "    ]\n",
    "    \n",
    "    features = {key: clean(row_data.get(key)) for key in inputs}\n",
    "    example = dspy.Example(features=features).with_inputs(\"features\")\n",
    "    if with_fuel:\n",
    "        example.fuel_kg = clean(row_data.get(\"fuel_kg\"))\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    df_to_dspy_examples(row, with_fuel=False)\n",
    "    for row in initial_df.iter_rows(named=True)\n",
    "]\n",
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6aaeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "randomized_examples = copy.deepcopy(examples)\n",
    "random.Random(42).shuffle(randomized_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e599bc6",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def float_metric(gold: dspy.Example, pred: dspy.Prediction, trace=None):\n",
    "    \"\"\"Return a scalar score (negative squared error) for the evaluator.\"\"\"\n",
    "    true_value = gold.fuel_kg\n",
    "    pred_value = pred.fuel_kg\n",
    "    if true_value is None or pred_value is None:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    squared_error = (true_value - pred_value) ** 2\n",
    "\n",
    "    if trace is None: # if we're doing evaluation or optimization\n",
    "        return -squared_error\n",
    "    else:  # During bootstrapping / trace collection we simply mark good demos.\n",
    "        return squared_error < 40000  # Squared error 200.\n",
    "    \n",
    "fake_example = dspy.Example(features={\"foo\": \"bar\"}, fuel_kg=500.0)\n",
    "fake_prediction = dspy.Prediction(features={\"foo\": \"bar\"}, fuel_kg=600.0)\n",
    "print(float_metric(fake_example, fake_prediction))  # error -100.0\n",
    "print(float_metric(fake_example, fake_prediction, trace='foo'))  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbca704",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0623d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_50 = Evaluate(\n",
    "    devset=randomized_examples[:50],\n",
    "    num_threads=50, \n",
    "    display_progress=True, \n",
    "    display_table=True\n",
    ")\n",
    "\n",
    "evaluator_500 = Evaluate(\n",
    "    devset=randomized_examples[:400],\n",
    "    num_threads=15, \n",
    "    display_progress=True, \n",
    "    display_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with dspy.context(lm=gpt_oss_120b):\n",
    "    eval_results = evaluator_500(fuel_cot, metric=float_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786bbf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(eval_results):\n",
    "    rmse = math.sqrt(abs(eval_results.score)/len(eval_results.results))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_rmse(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cdb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump in a csv file\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "results_list = []\n",
    "for result in eval_results.results:\n",
    "    true_value = result[0].fuel_burnt\n",
    "    reasoning = result[1].reasoning\n",
    "    pred_value = result[1].fuel_burnt\n",
    "    metric = result[2]\n",
    "    results_list.append({\n",
    "        \"true_value\": true_value,\n",
    "        \"reasoning\": reasoning,\n",
    "        \"pred_value\": pred_value,\n",
    "        \"metric\": metric\n",
    "    })\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_csv(\"eval_results.csv\", index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf789e9",
   "metadata": {},
   "source": [
    "## Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7093301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb36661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(examples, n_examples_per_batch = 10000):\n",
    "    list_of_batches = []\n",
    "    for i in tqdm(\n",
    "        range(0, len(examples), n_examples_per_batch)\n",
    "    ):\n",
    "        batch_range = examples[i : i + n_examples_per_batch]\n",
    "        list_of_batches.append(batch_range)\n",
    "    return list_of_batches\n",
    "\n",
    "list_of_batches = create_minibatch(examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_batch_files(list_of_batches: list):\n",
    "    \"\"\"Takes a list of minibatches and creates the files\"\"\"\n",
    "    list_of_artifacts = []\n",
    "    for (i, minibatch) in tqdm(enumerate(list_of_batches), total=len(list_of_batches)):\n",
    "        # 1. create a jsonl file\n",
    "        artifact = fuel_cot.create_batch_file(\n",
    "            minibatch,\n",
    "            input_file_path=os.path.join(OUTPUT_DIR, \"batches\", str(i)+\".jsonl\"),\n",
    "            # endpoint=\"/v1/chat/completions\"\n",
    "        )\n",
    "        list_of_artifacts.append(artifact)\n",
    "    return list_of_artifacts\n",
    "\n",
    "list_of_artifacts = create_batch_files(list_of_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3253eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. send it to the endpoint\n",
    "# DANGER :)\n",
    "async def send_batches(list_of_artifacts):\n",
    "    batch_handles = [\n",
    "        await fuel_cot.asubmit_batch_file(\n",
    "            artifact,\n",
    "            completion_window=\"24h\",\n",
    "        ) for artifact in list_of_artifacts\n",
    "    ]\n",
    "    return batch_handles\n",
    "\n",
    "batch_handles = await send_batches(list_of_artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. and finally retrieve the predictions\n",
    "responses = [\n",
    "    await fuel_cot.aretrieve_batch_predictions(\n",
    "        batch_handle.batch_id,\n",
    "        artifact,\n",
    "        custom_llm_provider=\"groq\",\n",
    "        download_output_path=os.path.join(OUTPUT_DIR, \"batches_output\", batch_handle.batch_id),  # optional, defaults next to metadata\n",
    "        return_failed_items=True,\n",
    "    ) for batch_handle, artifact in zip(batch_handles, list_of_artifacts)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c82bfb",
   "metadata": {},
   "source": [
    "## Create a df with input/preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5066bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "failures_to_retry = []\n",
    "\n",
    "for (preds, failures), artifact in zip(responses, list_of_artifacts):\n",
    "    examples = artifact.metadata[\"examples\"]\n",
    "    for idx, pred in enumerate(preds):\n",
    "        row = {\n",
    "            **examples[idx][\"inputs\"],              # original features\n",
    "            \"custom_id\": examples[idx][\"custom_id\"]\n",
    "        }\n",
    "        if pred is None:\n",
    "            row[\"fuel_kg\"] = None\n",
    "            row[\"reasoning\"] = None\n",
    "        else:\n",
    "            outputs = pred.toDict()\n",
    "            row[\"fuel_kg\"] = outputs[\"fuel_kg\"]\n",
    "            row[\"reasoning\"] = outputs[\"reasoning\"]\n",
    "        rows.append(row)\n",
    "\n",
    "    for failure in failures:\n",
    "        failures_to_retry.append(\n",
    "            examples[failure.index][\"inputs\"]\n",
    "        )\n",
    "\n",
    "final_df = pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def responses_to_dataframe(\n",
    "    responses,\n",
    "    artifacts,\n",
    "    *,\n",
    "    feature_col: str = \"features\",\n",
    "    prediction_cols: tuple[str, str] = (\"reasoning\", \"fuel_kg\"),\n",
    "    nested_feature_key: str = \"features\",  # the key inside artifact metadata\n",
    "    index_field: str = \"idx\",\n",
    "):\n",
    "    rows: list[dict] = []\n",
    "    failures_to_retry: list[dict] = []\n",
    "\n",
    "    for (preds, failures), artifact in zip(responses, artifacts):\n",
    "        examples = artifact.metadata[\"examples\"]\n",
    "        for idx, pred in enumerate(preds):\n",
    "            inputs = examples[idx][\"inputs\"]\n",
    "            nested_features = inputs.get(nested_feature_key, inputs)\n",
    "\n",
    "            row = {\n",
    "                feature_col: nested_features,                # keep the full feature dict\n",
    "                \"custom_id\": examples[idx][\"custom_id\"],\n",
    "                index_field: nested_features.get(index_field),\n",
    "            }\n",
    "\n",
    "            if pred is None:\n",
    "                for col in prediction_cols:\n",
    "                    row[col] = None\n",
    "            else:\n",
    "                outputs = pred.toDict()\n",
    "                for col in prediction_cols:\n",
    "                    row[col] = outputs.get(col)\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "        for failure in failures:\n",
    "            failures_to_retry.append(examples[failure.index][\"inputs\"])\n",
    "\n",
    "    final_df = pd.DataFrame(rows)\n",
    "    return final_df, failures_to_retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62771d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df, retry_payloads = responses_to_dataframe(\n",
    "    responses,\n",
    "    list_of_artifacts,\n",
    "    feature_col=\"features\",\n",
    "    prediction_cols=(\"reasoning\", \"fuel_kg\"),\n",
    "    nested_feature_key=\"features\",\n",
    "    index_field=\"idx\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc4f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5502624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dspy\n",
    "\n",
    "async def round_v2(\n",
    "    module: dspy.Module,\n",
    "    final_df: pd.DataFrame,\n",
    "    *,\n",
    "    feature_column: str = \"features\",\n",
    "    prediction_columns: tuple[str, str] = (\"reasoning\", \"fuel_kg\"),\n",
    "    index_column: str = \"idx\",\n",
    "    n_pred_per_batch: int = 10_000,\n",
    "    minimal_batch_threshold: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fill missing predictions in `final_df` by calling `module`.\n",
    "    Adds/updates `index_column` so you can join back to the original dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_example(payload):\n",
    "        if isinstance(payload, dspy.Example):\n",
    "            return payload\n",
    "        if isinstance(payload, dict):\n",
    "            nested = payload.get(\"features\", payload)\n",
    "            return dspy.Example(features=nested).with_inputs(\"features\")\n",
    "        raise TypeError(...)\n",
    "\n",
    "\n",
    "    def get_idx(payload):\n",
    "        if isinstance(payload, dspy.Example):\n",
    "            data = payload.inputs().toDict().get(\"features\", payload.inputs().toDict())\n",
    "        elif isinstance(payload, dict):\n",
    "            data = payload.get(\"features\", payload)\n",
    "        else:\n",
    "            return None\n",
    "        return data.get(\"idx\")\n",
    "\n",
    "\n",
    "    if index_column not in final_df.columns:\n",
    "        final_df[index_column] = pd.NA\n",
    "\n",
    "    missing_mask = final_df[prediction_columns[-1]].isna()\n",
    "    pending_idx = final_df.index[missing_mask].tolist()\n",
    "    if not pending_idx:\n",
    "        return final_df, 0, 0\n",
    "\n",
    "    pending_idx = pending_idx[:n_pred_per_batch]\n",
    "    indexed_examples = []\n",
    "    for row_idx in pending_idx:\n",
    "        payload = final_df.at[row_idx, feature_column]\n",
    "        example = to_example(payload)\n",
    "        # make sure we keep the original idx visible on the dataframe\n",
    "        idx_value = get_idx(payload)\n",
    "        if idx_value is not None:\n",
    "            final_df.at[row_idx, index_column] = idx_value\n",
    "        indexed_examples.append((row_idx, example))\n",
    "\n",
    "    examples = [ex for _, ex in indexed_examples]\n",
    "    newly_filled = 0\n",
    "\n",
    "    if len(examples) < minimal_batch_threshold:\n",
    "        predictions, failed_examples, _ = module.batch(\n",
    "            examples,\n",
    "            return_failed_examples=True,\n",
    "        )\n",
    "        for (row_idx, _), pred in zip(indexed_examples, predictions):\n",
    "            if pred is None:\n",
    "                continue\n",
    "            values = pred.toDict()\n",
    "            for col in prediction_columns:\n",
    "                final_df.at[row_idx, col] = values.get(col)\n",
    "            newly_filled += 1\n",
    "        remaining = len(pending_idx) - newly_filled\n",
    "        return final_df, newly_filled, remaining\n",
    "\n",
    "    preds, failures = await module.abatch(\n",
    "        examples,\n",
    "        return_failed_items=True,\n",
    "    )\n",
    "    for position, pred in enumerate(preds):\n",
    "        if pred is None:\n",
    "            continue\n",
    "        row_idx = indexed_examples[position][0]\n",
    "        values = pred.toDict()\n",
    "        for col in prediction_columns:\n",
    "            final_df.at[row_idx, col] = values.get(col)\n",
    "        newly_filled += 1\n",
    "\n",
    "    remaining = len(pending_idx) - newly_filled\n",
    "    return final_df, newly_filled, remaining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b67d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df, filled, remaining = await round_v2(\n",
    "    fuel_cot,\n",
    "    final_df,\n",
    "    n_pred_per_batch=10_000,\n",
    "    minimal_batch_threshold=200,\n",
    ")\n",
    "\n",
    "print(f\"Filled {filled} rows; {remaining} still NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d89cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file = pd.read_parquet(os.path.join(DATA_PATH, \"fuel_final_submission.parquet\"))\n",
    "submission_file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f02a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the idx column is unique in final_df (drop duplicates if you retried rows)\n",
    "pred_lookup = (\n",
    "    final_df\n",
    "    .dropna(subset=[\"fuel_kg\"])\n",
    "    .drop_duplicates(subset=[\"idx\"], keep=\"last\")\n",
    "    .set_index(\"idx\")[\"fuel_kg\"]\n",
    ")\n",
    "\n",
    "submission_file[\"fuel_kg\"] = submission_file[\"idx\"].map(pred_lookup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa174f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file.to_parquet(\"llm_submission.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fd53f",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5916f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lm in [gpt_oss_120b]:\n",
    "  cost = sum([x['cost'] for x in lm.history if x['cost'] is not None])\n",
    "  print(cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prc2025dspy (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
