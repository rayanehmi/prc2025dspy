{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87af93b9",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Goal: Link the flight data to fuel segments and chunk them for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "813aa905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/rayte/Work/prc2025dspy/data')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "DATA_PATH = Path.cwd().parent / \"data\"\n",
    "DATA_TYPE : Literal[\"train\", \"rank\", \"final\"] = \"train\"\n",
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22691c",
   "metadata": {},
   "source": [
    "## 1. Augment segments with select features\n",
    "\n",
    "Refer to DATA.md for explanations on the data.\n",
    "\n",
    "Let's link the features to each fuel segment.\n",
    "\n",
    "Since we are working with LLMs, we can save tokens by:\n",
    "- excluding features that are not used in their reasoning\n",
    "- summarizing features which have many data points, such as track_points and timestamps\n",
    "- round (and convert) numbers, especially those with high decimal precision\n",
    "- possibly exclude missing values (\"NaN\" --> \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a703eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "fuel_file_name = \"fuel_\" + (DATA_TYPE if DATA_TYPE == \"train\" else DATA_TYPE + \"_submission\")\n",
    "fuel_file_path = os.path.join(DATA_PATH, fuel_file_name + \".parquet\")\n",
    "flightlist_path = os.path.join(DATA_PATH, \"flightlist_\" + DATA_TYPE + \".parquet\")\n",
    "flights_folder_path = os.path.join(DATA_PATH, \"flights_\" + DATA_TYPE)\n",
    "airports_file_path = os.path.join(DATA_PATH, \"apt.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "703466a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 100\n",
    "OUTPUT_DIR = os.path.join(DATA_PATH, \"output_batches\", DATA_TYPE)\n",
    "TRAJECTORY_FOLDER = os.path.join(DATA_PATH, \"flights_train\") \n",
    "fuel_file_path = os.path.join(DATA_PATH, \"fuel_train.parquet\")\n",
    "flightlist_path = os.path.join(DATA_PATH, \"flightlist_train.parquet\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b71ad62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Found 11037 flights.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Load Metadata & Sanitize ---\n",
    "print(\"Loading metadata...\")\n",
    "fuel_df = pl.read_parquet(fuel_file_path)\n",
    "flightlist_df = pl.read_parquet(flightlist_path)\n",
    "\n",
    "skeleton_df = (\n",
    "    fuel_df\n",
    "    .join(flightlist_df, on=\"flight_id\", how=\"left\")\n",
    "    # FIX 1: Sanitize IDs and pre-calculate types\n",
    "    .with_columns([\n",
    "        pl.col(\"flight_id\").cast(pl.String).str.strip_chars(),\n",
    "        ((pl.col(\"end\") - pl.col(\"start\")).dt.total_minutes()).alias(\"duration_min\"),\n",
    "        (pl.col(\"origin_icao\") + \"-\" + pl.col(\"destination_icao\")).alias(\"route_icao\"),\n",
    "        ((pl.col(\"start\") - pl.col(\"takeoff\")) / (pl.col(\"landed\") - pl.col(\"takeoff\"))).alias(\"progress_pct\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "unique_flight_ids = skeleton_df[\"flight_id\"].unique().to_list()\n",
    "total_flights = len(unique_flight_ids)\n",
    "print(f\"Found {total_flights} flights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Batch Processing ---\n",
    "for i in tqdm(range(0, total_flights, BATCH_SIZE), desc=\"Processing Batches\"):\n",
    "    \n",
    "    batch_ids = unique_flight_ids[i : i + BATCH_SIZE]\n",
    "    batch_filename = os.path.join(OUTPUT_DIR, f\"batch_{i // BATCH_SIZE}.parquet\")\n",
    "    \n",
    "    if os.path.exists(batch_filename):\n",
    "        continue\n",
    "\n",
    "    valid_files = []\n",
    "    valid_ids_in_batch = []\n",
    "    \n",
    "    for fid in batch_ids:\n",
    "        path = os.path.join(TRAJECTORY_FOLDER, f\"{fid}.parquet\")\n",
    "        if os.path.exists(path):\n",
    "            valid_files.append(path)\n",
    "            valid_ids_in_batch.append(fid)\n",
    "\n",
    "    # Prepare the skeleton for this batch\n",
    "    current_skeleton = skeleton_df.filter(pl.col(\"flight_id\").is_in(batch_ids))\n",
    "\n",
    "    if valid_files:\n",
    "        try:\n",
    "            # Load Trajectories with Normalized Types\n",
    "            traj_lazy = (\n",
    "                pl.scan_parquet(valid_files)\n",
    "                .with_columns([\n",
    "                    pl.col(\"flight_id\").cast(pl.String).str.strip_chars(),\n",
    "                    # Force timestamps to Naive Microseconds to match fuel_df\n",
    "                    pl.col(\"timestamp\").dt.cast_time_unit(\"us\").dt.replace_time_zone(None)\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            # Prepare Skeleton for Join with Normalized Types\n",
    "            skeleton_lazy = (\n",
    "                current_skeleton.lazy()\n",
    "                .with_columns([\n",
    "                    pl.col(\"start\").dt.cast_time_unit(\"us\").dt.replace_time_zone(None),\n",
    "                    pl.col(\"end\").dt.cast_time_unit(\"us\").dt.replace_time_zone(None)\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            # Calculate Stats\n",
    "            stats_df = (\n",
    "                skeleton_lazy\n",
    "                .join(traj_lazy, on=\"flight_id\", how=\"inner\")\n",
    "                .filter(\n",
    "                    # Add 30s buffer to catch points for short/zero-duration segments\n",
    "                    (pl.col(\"timestamp\") >= pl.col(\"start\").dt.offset_by(\"-30s\")) & \n",
    "                    (pl.col(\"timestamp\") <= pl.col(\"end\").dt.offset_by(\"30s\"))\n",
    "                )\n",
    "                .sort(\"timestamp\")\n",
    "                .group_by(\"idx\")\n",
    "                .agg([\n",
    "                    pl.col(\"altitude\").first().alias(\"alt_start\"),\n",
    "                    pl.col(\"altitude\").last().alias(\"alt_end\"),\n",
    "                    pl.col(\"mach\").mean().alias(\"mach_avg\")\n",
    "                ])\n",
    "                .collect() \n",
    "            )\n",
    "\n",
    "            # Join back to ensure all rows are kept\n",
    "            final_batch = (\n",
    "                current_skeleton\n",
    "                .join(stats_df, on=\"idx\", how=\"left\")\n",
    "                .select([\n",
    "                    pl.col(\"idx\"),\n",
    "                    pl.col(\"flight_id\"),\n",
    "                    pl.col(\"aircraft_type\").alias(\"aircraft\"),\n",
    "                    pl.col(\"duration_min\"),\n",
    "                    pl.col(\"route_icao\"),\n",
    "                    pl.struct([\n",
    "                        pl.col(\"progress_pct\"),\n",
    "                        pl.col(\"alt_start\"),\n",
    "                        pl.col(\"alt_end\"),\n",
    "                        pl.col(\"mach_avg\")\n",
    "                    ]).alias(\"status\")\n",
    "                ])\n",
    "            )\n",
    "            final_batch.write_parquet(batch_filename)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i}: {e}\")\n",
    "\n",
    "    else:\n",
    "        # Fallback for batches with no files\n",
    "        final_batch = (\n",
    "            current_skeleton\n",
    "            .select([\n",
    "                pl.col(\"idx\"),\n",
    "                pl.col(\"flight_id\"),\n",
    "                pl.col(\"aircraft_type\").alias(\"aircraft\"),\n",
    "                pl.col(\"duration_min\"),\n",
    "                pl.col(\"route_icao\"),\n",
    "                pl.struct([\n",
    "                    pl.col(\"progress_pct\"),\n",
    "                    pl.lit(None).alias(\"alt_start\"),\n",
    "                    pl.lit(None).alias(\"alt_end\"),\n",
    "                    pl.lit(None).alias(\"mach_avg\")\n",
    "                ]).alias(\"status\")\n",
    "            ])\n",
    "        )\n",
    "        final_batch.write_parquet(batch_filename)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8d0653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one batch\n",
    "df = pl.read_parquet(os.path.join(OUTPUT_DIR, \"batch_0.parquet\"))\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prc2025dspy (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
