{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87af93b9",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Goal: Link the flight data to fuel segments and chunk them for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a5041d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow.dataset as ds\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "813aa905",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(os.getcwd(), \"data\")\n",
    "DATA_TYPE : Literal[\"train\", \"rank\", \"final\"] = \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22691c",
   "metadata": {},
   "source": [
    "## 1. Augment segments with select features\n",
    "\n",
    "Refer to DATA.md for explanations on the data.\n",
    "\n",
    "Let's link the features to each fuel segment.\n",
    "\n",
    "Since we are working with LLMs, we can save tokens by:\n",
    "- excluding features that are not used in their reasoning\n",
    "- summarizing features which have many data points, such as track_points and timestamps\n",
    "- round (and convert) numbers, especially those with high decimal precision\n",
    "- possibly exclude missing values (\"NaN\" --> \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a703eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_file_name = \"fuel_\" + (DATA_TYPE if DATA_TYPE == \"train\" else DATA_TYPE + \"_submission\")\n",
    "fuel_file_path = os.path.join(DATA_PATH, fuel_file_name + \".parquet\")\n",
    "flightlist_path = os.path.join(DATA_PATH, \"flightlist_\" + DATA_TYPE + \".parquet\")\n",
    "flights_folder_path = os.path.join(DATA_PATH, \"flights_\" + DATA_TYPE)\n",
    "airports_file_path = os.path.join(DATA_PATH, \"apt.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac542e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_df_test = pd.read_parquet(fuel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e976b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of unique flights\n",
    "print(fuel_df_test[\"flight_id\"].nunique())\n",
    "print(len(fuel_df_test))\n",
    "print(len(fuel_df_test) / fuel_df_test[\"flight_id\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc2c89b",
   "metadata": {},
   "source": [
    "Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2945d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_df = pl.read_parquet(fuel_file_path).lazy()\n",
    "airports_df = pl.read_parquet(airports_file_path).lazy()\n",
    "flightlist_df = pl.read_parquet(flightlist_path).lazy()\n",
    "\n",
    "flights_folder = pl.scan_parquet(os.path.join(flights_folder_path,\"*.parquet\"))\n",
    "\n",
    "skeleton_df = (\n",
    "    fuel_df\n",
    "    .join(flightlist_df, on=\"flight_id\", how=\"left\") \n",
    "    .with_columns([\n",
    "        # 1. Duration (in minutes)\n",
    "        ((pl.col(\"end\") - pl.col(\"start\")).dt.total_minutes()).alias(\"duration_min\"),\n",
    "        \n",
    "        # 2. Route (Origin-Dest)\n",
    "        (pl.col(\"origin_icao\") + \"-\" + pl.col(\"destination_icao\")).alias(\"route_icao\"),\n",
    "        \n",
    "        # 3. Progress Pct: (Segment Start - Takeoff) / (Landed - Takeoff)\n",
    "        (\n",
    "            (pl.col(\"start\") - pl.col(\"takeoff\")) / \n",
    "            (pl.col(\"landed\") - pl.col(\"takeoff\"))\n",
    "        ).alias(\"progress_pct\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "joined_df = (\n",
    "    skeleton_df\n",
    "    .join(flights_folder, on=\"flight_id\", how=\"left\")\n",
    "    # filter trajectory points to be within the fuel segment window\n",
    "    .filter(\n",
    "        (pl.col(\"timestamp\") >= pl.col(\"start\")) & \n",
    "        (pl.col(\"timestamp\") <= pl.col(\"end\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "final_lazy = (\n",
    "    joined_df\n",
    "    .sort(\"timestamp\")\n",
    "    .group_by(\"idx\")\n",
    "    .agg([\n",
    "        # Preserve the metadata columns \n",
    "        pl.col(\"aircraft_type\").first().alias(\"aircraft\"),\n",
    "        pl.col(\"duration_min\").first(),\n",
    "        pl.col(\"route_icao\").first(),\n",
    "        \n",
    "        # Create the 'status' struct\n",
    "        pl.struct([\n",
    "            pl.col(\"progress_pct\").first().alias(\"progress_pct\"),\n",
    "            pl.col(\"altitude\").first().alias(\"alt_start\"),\n",
    "            pl.col(\"altitude\").last().alias(\"alt_end\"),\n",
    "            pl.col(\"mach\").mean().alias(\"mach_avg\")\n",
    "        ]).alias(\"status\")\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5daaf1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = final_lazy.collect(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f4c12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfbe434",
   "metadata": {},
   "source": [
    "Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b71ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 100\n",
    "OUTPUT_DIR = os.path.join(DATA_PATH, \"output_batches\", DATA_TYPE)\n",
    "TRAJECTORY_FOLDER = os.path.join(DATA_PATH, \"flights_train\") \n",
    "fuel_file_path = os.path.join(DATA_PATH, \"fuel_train.parquet\")\n",
    "flightlist_path = os.path.join(DATA_PATH, \"flightlist_train.parquet\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. Load Metadata & Sanitize ---\n",
    "print(\"Loading metadata...\")\n",
    "fuel_df = pl.read_parquet(fuel_file_path)\n",
    "flightlist_df = pl.read_parquet(flightlist_path)\n",
    "\n",
    "skeleton_df = (\n",
    "    fuel_df\n",
    "    .join(flightlist_df, on=\"flight_id\", how=\"left\")\n",
    "    # FIX 1: Sanitize IDs and pre-calculate types\n",
    "    .with_columns([\n",
    "        pl.col(\"flight_id\").cast(pl.String).str.strip_chars(),\n",
    "        ((pl.col(\"end\") - pl.col(\"start\")).dt.total_minutes()).alias(\"duration_min\"),\n",
    "        (pl.col(\"origin_icao\") + \"-\" + pl.col(\"destination_icao\")).alias(\"route_icao\"),\n",
    "        ((pl.col(\"start\") - pl.col(\"takeoff\")) / (pl.col(\"landed\") - pl.col(\"takeoff\"))).alias(\"progress_pct\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "unique_flight_ids = skeleton_df[\"flight_id\"].unique().to_list()\n",
    "total_flights = len(unique_flight_ids)\n",
    "print(f\"Found {total_flights} flights. Processing...\")\n",
    "\n",
    "# --- 2. Batch Processing ---\n",
    "for i in tqdm(range(0, total_flights, BATCH_SIZE), desc=\"Processing Batches\"):\n",
    "    \n",
    "    batch_ids = unique_flight_ids[i : i + BATCH_SIZE]\n",
    "    batch_filename = os.path.join(OUTPUT_DIR, f\"batch_{i // BATCH_SIZE}.parquet\")\n",
    "    \n",
    "    if os.path.exists(batch_filename):\n",
    "        continue\n",
    "\n",
    "    valid_files = []\n",
    "    valid_ids_in_batch = []\n",
    "    \n",
    "    for fid in batch_ids:\n",
    "        path = os.path.join(TRAJECTORY_FOLDER, f\"{fid}.parquet\")\n",
    "        if os.path.exists(path):\n",
    "            valid_files.append(path)\n",
    "            valid_ids_in_batch.append(fid)\n",
    "\n",
    "    # Prepare the skeleton for this batch\n",
    "    current_skeleton = skeleton_df.filter(pl.col(\"flight_id\").is_in(batch_ids))\n",
    "\n",
    "    if valid_files:\n",
    "        try:\n",
    "            # Load Trajectories with Normalized Types\n",
    "            traj_lazy = (\n",
    "                pl.scan_parquet(valid_files)\n",
    "                .with_columns([\n",
    "                    pl.col(\"flight_id\").cast(pl.String).str.strip_chars(),\n",
    "                    # Force timestamps to Naive Microseconds to match fuel_df\n",
    "                    pl.col(\"timestamp\").dt.cast_time_unit(\"us\").dt.replace_time_zone(None)\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            # Prepare Skeleton for Join with Normalized Types\n",
    "            skeleton_lazy = (\n",
    "                current_skeleton.lazy()\n",
    "                .with_columns([\n",
    "                    pl.col(\"start\").dt.cast_time_unit(\"us\").dt.replace_time_zone(None),\n",
    "                    pl.col(\"end\").dt.cast_time_unit(\"us\").dt.replace_time_zone(None)\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            # Calculate Stats\n",
    "            stats_df = (\n",
    "                skeleton_lazy\n",
    "                .join(traj_lazy, on=\"flight_id\", how=\"inner\")\n",
    "                .filter(\n",
    "                    # Add 30s buffer to catch points for short/zero-duration segments\n",
    "                    (pl.col(\"timestamp\") >= pl.col(\"start\").dt.offset_by(\"-30s\")) & \n",
    "                    (pl.col(\"timestamp\") <= pl.col(\"end\").dt.offset_by(\"30s\"))\n",
    "                )\n",
    "                .sort(\"timestamp\")\n",
    "                .group_by(\"idx\")\n",
    "                .agg([\n",
    "                    pl.col(\"altitude\").first().alias(\"alt_start\"),\n",
    "                    pl.col(\"altitude\").last().alias(\"alt_end\"),\n",
    "                    pl.col(\"mach\").mean().alias(\"mach_avg\")\n",
    "                ])\n",
    "                .collect() \n",
    "            )\n",
    "\n",
    "            # Join back to ensure all rows are kept\n",
    "            final_batch = (\n",
    "                current_skeleton\n",
    "                .join(stats_df, on=\"idx\", how=\"left\")\n",
    "                .select([\n",
    "                    pl.col(\"idx\"),\n",
    "                    pl.col(\"flight_id\"),\n",
    "                    pl.col(\"aircraft_type\").alias(\"aircraft\"),\n",
    "                    pl.col(\"duration_min\"),\n",
    "                    pl.col(\"route_icao\"),\n",
    "                    pl.struct([\n",
    "                        pl.col(\"progress_pct\"),\n",
    "                        pl.col(\"alt_start\"),\n",
    "                        pl.col(\"alt_end\"),\n",
    "                        pl.col(\"mach_avg\")\n",
    "                    ]).alias(\"status\")\n",
    "                ])\n",
    "            )\n",
    "            final_batch.write_parquet(batch_filename)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i}: {e}\")\n",
    "\n",
    "    else:\n",
    "        # Fallback for batches with no files\n",
    "        final_batch = (\n",
    "            current_skeleton\n",
    "            .select([\n",
    "                pl.col(\"idx\"),\n",
    "                pl.col(\"flight_id\"),\n",
    "                pl.col(\"aircraft_type\").alias(\"aircraft\"),\n",
    "                pl.col(\"duration_min\"),\n",
    "                pl.col(\"route_icao\"),\n",
    "                pl.struct([\n",
    "                    pl.col(\"progress_pct\"),\n",
    "                    pl.lit(None).alias(\"alt_start\"),\n",
    "                    pl.lit(None).alias(\"alt_end\"),\n",
    "                    pl.lit(None).alias(\"mach_avg\")\n",
    "                ]).alias(\"status\")\n",
    "            ])\n",
    "        )\n",
    "        final_batch.write_parquet(batch_filename)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8d0653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one batch\n",
    "df = pl.read_parquet(os.path.join(OUTPUT_DIR, \"batch_0.parquet\"))\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
