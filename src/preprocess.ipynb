{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87af93b9",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Goal: Link the flight data to fuel segments and chunk them for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "813aa905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/rayte/Work/prc2025dspy/data')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "DATA_PATH = Path.cwd().parent / \"data\"\n",
    "DATA_TYPE : Literal[\"train\", \"rank\", \"final\"] = \"train\"\n",
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22691c",
   "metadata": {},
   "source": [
    "## 1. Augment segments with select features\n",
    "\n",
    "Refer to DATA.md for explanations on the data.\n",
    "\n",
    "Let's link the features to each fuel segment.\n",
    "\n",
    "Since we are working with LLMs, we can save tokens by:\n",
    "- excluding features that are not used in their reasoning\n",
    "- summarizing features which have many data points, such as track_points and timestamps\n",
    "- round (and convert) numbers, especially those with high decimal precision\n",
    "- possibly exclude missing values (\"NaN\" --> \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a703eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "fuel_file_name = \"fuel_\" + (DATA_TYPE if DATA_TYPE == \"train\" else DATA_TYPE + \"_submission\")\n",
    "fuel_file_path = os.path.join(DATA_PATH, fuel_file_name + \".parquet\")\n",
    "flightlist_path = os.path.join(DATA_PATH, \"flightlist_\" + DATA_TYPE + \".parquet\")\n",
    "flights_folder_path = os.path.join(DATA_PATH, \"flights_\" + DATA_TYPE)\n",
    "airports_file_path = os.path.join(DATA_PATH, \"apt.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "703466a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 100\n",
    "OUTPUT_DIR = os.path.join(DATA_PATH, \"output_batches\", DATA_TYPE)\n",
    "TRAJECTORY_FOLDER = flights_folder_path\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "159ebae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Found 11037 flights.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Load Metadata & Sanitize ---\n",
    "print(\"Loading metadata...\")\n",
    "fuel_df = pl.read_parquet(fuel_file_path)\n",
    "flightlist_df = pl.read_parquet(flightlist_path)\n",
    "\n",
    "duration_minutes_expr = ((pl.col(\"end\") - pl.col(\"start\")).dt.total_seconds() / 60).cast(pl.Float64)\n",
    "duration_display_expr = (\n",
    "    pl.when(duration_minutes_expr >= 1)\n",
    "    .then(duration_minutes_expr.floor().cast(pl.Int64))\n",
    "    .otherwise(duration_minutes_expr.round(2))\n",
    ")\n",
    "\n",
    "skeleton_df = (\n",
    "    fuel_df\n",
    "    .join(flightlist_df, on=\"flight_id\", how=\"left\")\n",
    "    # FIX 1: Sanitize IDs and pre-calculate types\n",
    "    .with_columns([\n",
    "        pl.col(\"flight_id\").cast(pl.String).str.strip_chars(),\n",
    "        pl.col(\"fuel_kg\").alias(\"fuel_burnt\"),\n",
    "        pl.format(\"{}min\", duration_display_expr).alias(\"duration_min\"),\n",
    "        (pl.col(\"origin_icao\") + \"-\" + pl.col(\"destination_icao\")).alias(\"route_icao\"),\n",
    "        ((pl.col(\"start\") - pl.col(\"takeoff\")) / (pl.col(\"landed\") - pl.col(\"takeoff\"))).alias(\"progress_pct\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "unique_flight_ids = skeleton_df[\"flight_id\"].unique().to_list()\n",
    "total_flights = len(unique_flight_ids)\n",
    "print(f\"Found {total_flights} flights.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b71ad62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 111/111 [00:23<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Batch Processing ---\n",
    "ALTITUDE_SLICE_METERS = 25\n",
    "\n",
    "def round_up_to_slice(expr, slice_size=ALTITUDE_SLICE_METERS):\n",
    "    return expr.truediv(slice_size).ceil().mul(slice_size)\n",
    "\n",
    "batch_select_columns = [\n",
    "    pl.col(\"idx\"),\n",
    "    pl.col(\"fuel_burnt\"),\n",
    "    pl.col(\"flight_id\"),\n",
    "    pl.col(\"aircraft_type\").alias(\"aircraft\"),\n",
    "    pl.col(\"duration_min\"),\n",
    "    pl.col(\"route_icao\"),\n",
    "    pl.col(\"alt_start\"),\n",
    "    pl.col(\"alt_end\"),\n",
    "    pl.col(\"peak_altitude\"),\n",
    "    pl.col(\"ground_speed_start\"),\n",
    "    pl.col(\"ground_speed_end\"),\n",
    "    pl.col(\"vertical_rate_start\"),\n",
    "    pl.col(\"vertical_rate_end\"),\n",
    "]\n",
    "\n",
    "metric_column_defaults = [\n",
    "    pl.lit(None).alias(\"alt_start\"),\n",
    "    pl.lit(None).alias(\"alt_end\"),\n",
    "    pl.lit(None).alias(\"peak_altitude\"),\n",
    "    pl.lit(None).alias(\"ground_speed_start\"),\n",
    "    pl.lit(None).alias(\"ground_speed_end\"),\n",
    "    pl.lit(None).alias(\"vertical_rate_start\"),\n",
    "    pl.lit(None).alias(\"vertical_rate_end\"),\n",
    "]\n",
    "\n",
    "for i in tqdm(range(0, total_flights, BATCH_SIZE), desc=\"Processing Batches\"):\n",
    "    \n",
    "    batch_ids = unique_flight_ids[i : i + BATCH_SIZE]\n",
    "    batch_filename = os.path.join(OUTPUT_DIR, f\"batch_{i // BATCH_SIZE}.parquet\")\n",
    "    \n",
    "    if os.path.exists(batch_filename):\n",
    "        continue\n",
    "\n",
    "    valid_files = []\n",
    "    valid_ids_in_batch = []\n",
    "    \n",
    "    for fid in batch_ids:\n",
    "        path = os.path.join(TRAJECTORY_FOLDER, f\"{fid}.parquet\")\n",
    "        if os.path.exists(path):\n",
    "            valid_files.append(path)\n",
    "            valid_ids_in_batch.append(fid)\n",
    "\n",
    "    # Prepare the skeleton for this batch\n",
    "    current_skeleton = skeleton_df.filter(pl.col(\"flight_id\").is_in(batch_ids))\n",
    "\n",
    "    if valid_files:\n",
    "        try:\n",
    "            # Load Trajectories with Normalized Types\n",
    "            traj_lazy = (\n",
    "                pl.scan_parquet(valid_files)\n",
    "                .with_columns([\n",
    "                    pl.col(\"flight_id\").cast(pl.String).str.strip_chars(),\n",
    "                    # Force timestamps to Naive Microseconds to match fuel_df\n",
    "                    pl.col(\"timestamp\").dt.cast_time_unit(\"us\").dt.replace_time_zone(None)\n",
    "                ])\n",
    "            )\n",
    "            \n",
    "            # Prepare Skeleton for Join with Normalized Types\n",
    "            skeleton_lazy = (\n",
    "                current_skeleton.lazy()\n",
    "                .with_columns([\n",
    "                    pl.col(\"start\").dt.cast_time_unit(\"us\").dt.replace_time_zone(None),\n",
    "                    pl.col(\"end\").dt.cast_time_unit(\"us\").dt.replace_time_zone(None)\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            # Calculate Stats\n",
    "            stats_df = (\n",
    "                skeleton_lazy\n",
    "                .join(traj_lazy, on=\"flight_id\", how=\"inner\")\n",
    "                .filter(\n",
    "                    # Add 30s buffer to catch points for short/zero-duration segments\n",
    "                    (pl.col(\"timestamp\") >= pl.col(\"start\").dt.offset_by(\"-30s\")) & \n",
    "                    (pl.col(\"timestamp\") <= pl.col(\"end\").dt.offset_by(\"30s\"))\n",
    "                )\n",
    "                .group_by(\"idx\")\n",
    "                .agg([\n",
    "                    round_up_to_slice(pl.col(\"altitude\").sort_by(\"timestamp\").first()).alias(\"alt_start\"),\n",
    "                    round_up_to_slice(pl.col(\"altitude\").sort_by(\"timestamp\").last()).alias(\"alt_end\"),\n",
    "                    round_up_to_slice(pl.col(\"altitude\").max()).alias(\"peak_altitude\"),\n",
    "                    pl.col(\"groundspeed\").sort_by(\"timestamp\").first().alias(\"ground_speed_start\"),\n",
    "                    pl.col(\"groundspeed\").sort_by(\"timestamp\").last().alias(\"ground_speed_end\"),\n",
    "                    pl.col(\"vertical_rate\").sort_by(\"timestamp\").first().alias(\"vertical_rate_start\"),\n",
    "                    pl.col(\"vertical_rate\").sort_by(\"timestamp\").last().alias(\"vertical_rate_end\"),\n",
    "                ])\n",
    "                .collect() \n",
    "            )\n",
    "\n",
    "            # Join back to ensure all rows are kept\n",
    "            final_batch = (\n",
    "                current_skeleton\n",
    "                .join(stats_df, on=\"idx\", how=\"left\")\n",
    "                .select(batch_select_columns)\n",
    "            )\n",
    "            final_batch.write_parquet(batch_filename)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i}: {e}\")\n",
    "\n",
    "    else:\n",
    "        # Fallback for batches with no files\n",
    "        final_batch = (\n",
    "            current_skeleton\n",
    "            .with_columns(metric_column_defaults)\n",
    "            .select(batch_select_columns)\n",
    "        )\n",
    "        final_batch.write_parquet(batch_filename)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "195450c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 111 files into c:\\Users\\rayte\\Work\\prc2025dspy\\data\\output_batches\\train\\complete.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Combine Batches Utility ---\n",
    "BATCH_PATTERN_DEFAULT = \"batch_*.parquet\"\n",
    "\n",
    "def combine_batches(output_name=\"complete.parquet\", pattern=BATCH_PATTERN_DEFAULT, overwrite=False, dry_run=False):\n",
    "    output_dir = Path(OUTPUT_DIR) if not isinstance(OUTPUT_DIR, Path) else OUTPUT_DIR\n",
    "    batch_files = sorted(\n",
    "        path for path in output_dir.glob(pattern)\n",
    "        if path.is_file() and path.name != output_name\n",
    "    )\n",
    "    if not batch_files:\n",
    "        print(f\"No files matching pattern '{pattern}' in {output_dir}.\")\n",
    "        return\n",
    "    output_path = output_dir / output_name\n",
    "    if output_path.exists() and not overwrite:\n",
    "        raise FileExistsError(\n",
    "            f\"Output {output_path} exists. Pass overwrite=True to replace it.\"\n",
    "        )\n",
    "    if dry_run:\n",
    "        print(f\"[dry-run] Would combine {len(batch_files)} files into {output_path}\")\n",
    "        for path in batch_files:\n",
    "            print(f\" - {path.name}\")\n",
    "        return\n",
    "    pl.scan_parquet([str(path) for path in batch_files]).sink_parquet(str(output_path))\n",
    "    print(f\"Combined {len(batch_files)} files into {output_path}\")\n",
    "\n",
    "\n",
    "combine_batches(overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e830f5d",
   "metadata": {},
   "source": [
    "## DSPy\n",
    "\n",
    "Note: I use my own fork of DSPy because I had to implement asynchronous batching: https://github.com/rayanehmi/dspy/tree/feat/async_batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "idx = 17909\n",
    "fuel_df = pd.read_parquet(fuel_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prc2025dspy (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
